{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations executed in parallel. A sorting network implements  a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$ (sorting networks for non-power-of-2 sizes are possible but not trickier).\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network for $n=2^k$ elements has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a $k$ element vector into sorted order.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification, but properly `logsumexp(a,b)`) as the continuous approximation to the `max(a,b)` function:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b) \\approx \\max(a,b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin(a,b)` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b}) \\approx \\min(a,b).$$ More numerically stably we can write: \n",
    "\n",
    "$$\\text{softmin}(a,b) = a + b - \\text{softmax}(a,b).$$\n",
    "\n",
    "These functions aren't equal to max and min, but are relatively close for numbers in some range, and differentiable. Note that we now have a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{softcswap}(a,b) = [\\text{softmin}(a,b), \\text{softmin}(a,b)]$$\n",
    "\n",
    "#### Smoothmax\n",
    "Alternatively, we can use [smoothmax](https://en.wikipedia.org/wiki/Smooth_maximum):\n",
    "\n",
    "$$\\text{smoothmax}(a,b) = \\frac{a (e^{\\alpha a}) + b (e^{\\alpha b})}{e^{\\alpha a}+e^{\\alpha b}}  \\approx \\max(a,b).$$  This has an adjustable smoothness parameter $\\alpha$, with exact maximum as $\\alpha \\rightarrow \\infty$ and pure averaging as $\\alpha \\rightarrow 0$.\n",
    "\n",
    "## Differentiable sorting\n",
    "\n",
    "For each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides which can be done simultaneously. We can select the relevant elements of the vector as a multiply with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ and $R \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{n}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ and $R' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i[\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i})] + R'_i[\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})]$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network, which are fixed for a given $n$. \n",
    "\n",
    "### Indexed form\n",
    "The matrices just implement permuations, so they can be replaced by reindexing, which requires only $2n$ indexing operations per layer, instead of $2 n \\times n$ matrix multiplies. See the **Indexed form** section below.\n",
    "\n",
    "## Example\n",
    "\n",
    "To sort four elements, we have a network like:\n",
    "\n",
    "    0  1  2  3  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕<<┙  \n",
    "    ┕>>>>>┙  │  \n",
    "    │  │  │  │  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕>>┙  \n",
    "    \n",
    "This is equivalent to: \n",
    "\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[3], x[2] = cswap(x[2], x[3])\n",
    "    x[0], x[2] = cswap(x[0], x[2])\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[2], x[3] = cswap(x[2], x[3])\n",
    "    \n",
    "where `cswap(a,b) = (min(a,b), max(a,b))`\n",
    "\n",
    "Replacing the indexing with matrix multiplies and `cswap` with a `softcswap = (softmin(a,b), softmax(a,b))` we then have the differentiable form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0>1\t 2<3\t 4>5\t 6<7\t 8>9\t10<11\t12>13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>2\t 1>3\t 4<6\t 5<7\t 8>10\t 9>11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4<5\t 6<7\t 8>9\t10>11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8<12\t 9<13\t10<14\t11<15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8<10\t 9<11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8<9\t10<11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>8\t 1>9\t 2>10\t 3>11\t 4>12\t 5>13\t 6>14\t 7>15\t\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8>12\t 9>13\t10>14\t11>15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8>10\t 9>11\t12>14\t13>15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8>9\t10>11\t12>13\t14>15\t\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from differentiable_sorting import bitonic_network, pretty_bitonic_network\n",
    "\n",
    "def neat_vec(n):\n",
    "    # print a vector neatly    \n",
    "    return \"\\t\".join([f\"{x:6.2f}\" for x in n])\n",
    "\n",
    "# the comparisons should match the diagram at the top of the notebook\n",
    "# each line represents one pair of matrix multiplies\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  1  2  3  4  5  6  7 \n",
      " ╭──╯  │  │  │  │  │  │ \n",
      " │  │  ╰──╮  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │ \n",
      " │  │  │  │  │  │  ╰──╮ \n",
      " ╭─────╯  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │ \n",
      " │  │  │  │  ╰─────╮  │ \n",
      " │  │  │  │  │  ╰─────╮ \n",
      " ╭──╯  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │ \n",
      " │  │  │  │  ╰──╮  │  │ \n",
      " │  │  │  │  │  │  ╰──╮ \n",
      " ╭───────────╯  │  │  │ \n",
      " │  ╭───────────╯  │  │ \n",
      " │  │  ╭───────────╯  │ \n",
      " │  │  │  ╭───────────╯ \n",
      " ╭─────╯  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │ \n",
      " │  │  │  │  ╭─────╯  │ \n",
      " │  │  │  │  │  ╭─────╯ \n",
      " ╭──╯  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │ \n",
      " │  │  │  │  │  │  ╭──╯ \n"
     ]
    }
   ],
   "source": [
    "# print the sorting network. The wires \"going over\" represent where the larger of the two values should go in each exchange.\n",
    "pretty_bitonic_network(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sorting\n",
    "import autograd.numpy as np # we can use plain numpy as well (but can't take grad!)\n",
    "np.set_printoptions(precision=2, suppress=2)\n",
    "\n",
    "from differentiable_sorting import bitonic_matrices, diff_sort, diff_argsort\n",
    "from differentiable_sorting import softmax, smoothmax, softmax_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing sorting network\n",
    "First, test bitonic sorting with exact (non-differentiable) maximum; this should work exactly as regular sorting does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.00\t 10.00\t 28.00\t 33.00\t 34.00\t 38.00\t 70.00\t146.00\n",
      "  0.00\t  7.00\t  9.00\t 12.00\t 18.00\t 34.00\t168.00\t184.00\n",
      " 16.00\t 23.00\t 43.00\t 99.00\t103.00\t124.00\t152.00\t197.00\n",
      " 21.00\t 36.00\t 60.00\t 84.00\t 92.00\t 94.00\t118.00\t141.00\n",
      "102.00\t109.00\t115.00\t125.00\t159.00\t169.00\t175.00\t185.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "for i in range(5):\n",
    "    # these should all be in sorted order\n",
    "    test = np.random.randint(0, 200, 8)\n",
    "    sortd = diff_sort(matrices, test, softmax=np.maximum)\n",
    "    assert(np.allclose(sortd, np.sort(test))) # check sorting matches expected\n",
    "    print(neat_vec(sortd))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test\n",
    "Now we can replace `np.maximum` with `softmax` or `smoothmax`. This is approximately the same, *for reasonable values of the input* (typically ~1-300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax sorting    -154.00\t-57.00\t-41.05\t-37.95\t-29.00\t112.00\t125.00\t153.00\n",
      "Smoothmax sorting  -154.00\t-57.00\t-40.86\t-38.14\t-29.00\t112.00\t125.00\t153.00\n",
      "Exact sorting      -154.00\t-57.00\t-41.00\t-38.00\t-29.00\t112.00\t125.00\t153.00\n",
      "\n",
      "Softmax sorting    -190.00\t-122.00\t-52.00\t-44.00\t 50.00\t103.00\t156.00\t168.00\n",
      "Smoothmax sorting  -190.00\t-122.00\t-51.99\t-44.01\t 50.00\t103.00\t156.00\t168.00\n",
      "Exact sorting      -190.00\t-122.00\t-52.00\t-44.00\t 50.00\t103.00\t156.00\t168.00\n",
      "\n",
      "Softmax sorting    -152.00\t-127.00\t-95.00\t-22.00\t  4.00\t 47.00\t 87.00\t198.00\n",
      "Smoothmax sorting  -152.00\t-127.00\t-95.00\t-22.00\t  4.00\t 47.00\t 87.00\t198.00\n",
      "Exact sorting      -152.00\t-127.00\t-95.00\t-22.00\t  4.00\t 47.00\t 87.00\t198.00\n",
      "\n",
      "Softmax sorting    -183.00\t-127.00\t-119.00\t-31.13\t-28.87\t109.00\t149.00\t163.00\n",
      "Smoothmax sorting  -183.00\t-127.00\t-119.00\t-30.76\t-29.24\t109.00\t149.00\t163.00\n",
      "Exact sorting      -183.00\t-127.00\t-119.00\t-31.00\t-29.00\t109.00\t149.00\t163.00\n",
      "\n",
      "Softmax sorting    -155.00\t-78.00\t-70.00\t-64.00\t-22.02\t-17.98\t 54.00\t107.00\n",
      "Smoothmax sorting  -155.00\t-78.00\t-69.99\t-64.01\t-21.93\t-18.07\t 54.00\t107.00\n",
      "Exact sorting      -155.00\t-78.00\t-70.00\t-64.00\t-22.00\t-18.00\t 54.00\t107.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differentiable sorting \n",
    "\n",
    "matrices = bitonic_matrices(8) \n",
    "\n",
    "for i in range(5):\n",
    "    # note the range here is chosen to work well: small values will not work well! \n",
    "    test = np.random.randint(-200,200,8)\n",
    "    print(\"Softmax sorting   \", neat_vec(diff_sort(matrices, test, softmax=softmax)))\n",
    "    print(\"Smoothmax sorting \", neat_vec(diff_sort(matrices, test, softmax=smoothmax)))\n",
    "    print(\"Exact sorting     \", neat_vec(diff_sort(matrices, test, softmax=np.maximum)))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive-only sorting\n",
    "Slightly better accuracy can be achieved with normalised softmax *if* all elements are strictly positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax sorting      2.39\t  3.92\t  4.71\t  6.11\t  5.93\t  7.32\t  7.59\t  9.02\n",
      "+Softmax sorting     2.42\t  3.91\t  4.71\t  6.10\t  5.93\t  7.32\t  7.59\t  9.01\n",
      "Exact sorting        3.00\t  4.00\t  6.00\t  6.00\t  7.00\t  7.00\t  7.00\t  7.00\n",
      "\n",
      "Softmax sorting      0.34\t  2.04\t  2.44\t  3.85\t  4.30\t  5.74\t  6.34\t  7.95\n",
      "+Softmax sorting     0.62\t  1.95\t  2.39\t  3.76\t  4.28\t  5.72\t  6.33\t  7.95\n",
      "Exact sorting        2.00\t  2.00\t  2.00\t  4.00\t  4.00\t  6.00\t  6.00\t  7.00\n",
      "\n",
      "Softmax sorting     -0.54\t  0.85\t  1.20\t  2.59\t  2.60\t  4.00\t  5.45\t  6.84\n",
      "+Softmax sorting     0.14\t  0.77\t  1.14\t  2.34\t  2.49\t  3.87\t  5.43\t  6.82\n",
      "Exact sorting        1.00\t  1.00\t  2.00\t  2.00\t  2.00\t  3.00\t  6.00\t  6.00\n",
      "\n",
      "Softmax sorting      0.31\t  1.81\t  2.47\t  3.86\t  4.42\t  5.85\t  6.40\t  7.88\n",
      "+Softmax sorting     0.57\t  1.72\t  2.43\t  3.77\t  4.40\t  5.84\t  6.39\t  7.87\n",
      "Exact sorting        1.00\t  2.00\t  3.00\t  3.00\t  5.00\t  6.00\t  6.00\t  7.00\n",
      "\n",
      "Softmax sorting     -0.20\t  1.19\t  1.98\t  3.41\t  4.04\t  5.76\t  6.58\t  8.24\n",
      "+Softmax sorting     0.26\t  1.06\t  1.88\t  3.25\t  4.00\t  5.73\t  6.57\t  8.24\n",
      "Exact sorting        1.00\t  1.00\t  2.00\t  3.00\t  4.00\t  6.00\t  7.00\t  7.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    # softmax works less well in this range\n",
    "    test = np.random.randint(1,8,8)\n",
    "    print(\"Softmax sorting   \", neat_vec(diff_sort(matrices, test, softmax=softmax)))\n",
    "    print(\"+Softmax sorting  \", neat_vec(diff_sort(matrices, test, softmax=lambda a,b: softmax(a,b,normalize=1))))\n",
    "    print(\"Exact sorting     \", neat_vec(diff_sort(matrices, test, softmax=np.maximum)))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting complex numbers\n",
    "The softmax/smoothmax functions are analytic as well as differentiable, so \n",
    "we can also \"sort\" complex numbers. This approximately sorts them by the real part with smoothmax. This isn't a very useful operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19a52e3c1d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEKCAYAAADTgGjXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X18VPW17/HPSkIQBCMoIII60AYBQSh6kKpVlFbF9Iqe120LatVqy0Gh1lPb26i0zLXWxp7a01qtig+nPmK1WqHGig9t0N76BAiKPAjqoIEIqDiCYCCTdf+YHR1gAsMkM3uSfN+v17yy99p7ZhZ7wqzs3++3f9vcHRERkWwUhZ2AiIi0XSoiIiKSNRURERHJmoqIiIhkTUVERESypiIiIiJZUxEREZGsqYiIiEjWVERERCRrJWEnINIeRSqr9wGeBTqT/H/251hVxYyWvKaZ1QC4+9iW5ifSWnQm0o5FKqtPi1RWr4hUVq+KVFZXhp1PB1MPnByrqhgBjAROi1RWjwk5J5FWpyLSTkUqq4uBm4DxwFBgUqSyemi4WXUcsaoKj1VVbA5WOwUPTVQn7Y6as9qv0cCqWFXFWwCRyuoHgAnA0lCz6kCCQr4A+CJwU6yq4sWQUxJpdToTab/6Ae+mrNcGMcmTWFVFIlZVMRLoD4yOVFYPCzsnkdYW2pmImR0C3A0cBDQCM939d2bWE/gTEAFiwDfdfaOZGfA74HRgC3CBuy8MI/c2wtLE1JwSglhVxUeRyuoa4DRgScjptDsLFizoXVJScjswDP1hvDcagSUNDQ3fPeqoo9Zn+yJhNmc1AJe7+0Iz6w4sMLOngAuAZ9y9yswqgUrgJyTb9suDxzHAzcFPSa8WOCRlvT+wNqRcOpxIZXUvYHtQQLoAXwWuCzmtdqmkpOT2gw46aEivXr02FhUV6Q+lDDU2NtqGDRuGvvfee7cDZ2T7OqEVEXevA+qC5U1mtoxkc8sEYGyw211ADckiMgG425N30XrBzPY3s77B68iuXgbKI5XVA4A1wETg7HBT6lD6AncF/SJFwIOxqorHQs6pvRqmArL3ioqKvFevXvH33nuvRc2sBdGxbmYR4EvAi0CfpsLg7nVm1jvYrbk2/l2KiJlNBiYHqzPdfWZuMi9csaqKhkhl9TRgLlAM3Bmrqng95LQ6jFhVxaskf6cl94pUQLITHLcWNQGGXkTMrBvwMHCZu3+c7PpIv2uaWNpfnKBodLjCsbNYVcXjwONh5yEi7VeonVBm1olkAbnP3R8JwuvMrG+wvS/Q1OGjNn4paLOmT7ixbkakoXFGmdfNiDTMmj7hxh12iJYdQrTsH0TLlhEte51o2Q9CSlVaWXFx8VGDBw8eWl5efsT48eMHbtq0KbTv1quvvrp3Pt8/tH9oMNrqDmCZu/8mZdMc4Pxg+Xxgdkr8PEsaA8TVH7KjPX6JAUTL7iRatp5omUYJtaJZ0yfceGbxv6b2tY3FRQZ9bWPxmcX/mrrTZ9AAXE40PgQYA0wlWqYLQNuBzp07Ny5fvnzpypUrX+/UqZNff/31vVK3NzY2kkgkcp5HQ0MDt956a5/Nmze3/yICHAd8GzjZzBYFj9OBKuBrZrYS+FqwDslmmbeAVcBtwCUh5FywMvwSA/gjyaGm0orGFi+e0sW27RDrYtsYW7x4ymeBaLyOaHxhsLwJaBpMInl07wure47+xdPDB1RWHzX6F08Pv/eF1T1b8/WPP/74zatWreq8YsWK0oEDBx5x7rnnHnrEEUcMffPNN0sfeeSR/UaOHDl46NChQ8aPHz8wHo/v8h28evXqTkcfffThTWc2TzzxRDeAW2+9teegQYOGlpeXH3HxxRd/9nvTtWvXL1122WUHH3nkkYMrKyv7rl+/vtOJJ5446JhjjhnUmv+u5oRWRNz9n+5u7n6ku48MHo+7+wfuPs7dy4OfHwb7u7tPdfcvuPtwd58fVu6FKKMvMYBo/Fngwzym1iH0YWPx3sSJlkX4fDCJ5Mm9L6zu+fPHlh62flN9qQPrN9WX/vyxpYe1ViHZvn07c+fO3W/48OFbAWKx2D7f+c53Pli2bNnS7t27N1577bV9n3322TeWLl26bNSoUVt+/vOf99n5Ne68886e48aNiy9fvnzpsmXLXj/mmGO2xGKxTtFotF9NTc0bS5cuff2VV17Z95577tkfYOvWrUXDhg3b+uqrry7/9a9/Xde7d+/t8+bNe+PFF198ozX+TXuiC3Paib3+EpNWtY4eadsq0sajZZ8NJiEa/zjHqUmKG55Z2a++oXGH7736hsaiG55Z2aIzwvr6+qLBgwcPHT58+ND+/ftv+8EPfvA+QN++fbeNGzfuE4Campp933zzzX1Gjx49ePDgwUMfeOCBA955553SnV9rzJgxn8yaNevAH/7whwe/9NJLXXr06NH4z3/+c98xY8ZsOvjggxs6derEt771rQ/nzZvXDaC4uJgLLrhgY0vyb4nQR2dJ61hHj0TfNAUjGZdcq0mMuOXM4n9NTT0b3Oql1CRG3DIpdcdo2WeDSYjGH0HyasOm+l2+tHcXz1RTn8jO8a5duzY2Lbs7xx9//Md//etf307d5+9///u+l1xyyWEAP/3pT9ecc8458WeffXbFww8/XHbBBRcMuPTSS9eVlZU126FSWlraWFIS3le5zkTaiZrEiFu2+o7/D5q+xEJKqUOZdM3saY8mjr2pznskGh3qvEfi0cSxN026Zva0z3aKln02mIRo/DfNvpjkTK/unbftTbw1jR079pP58+d3W7JkSWeATZs2Fb366qudTz755E+WL1++dPny5UvPOeec+BtvvFHar1+/7Zdffvn755577vsLFy7sesIJJ3zy4osvdq+rqytpaGjgoYce6jl27NjN6d5n3333TaTra8kVnYm0E5OumT1t1vQJjC1ePKUPG4vX0SNRkxhxyw5fYpJTwbGeBsnL1SftukvTYJLXiJYtCmJXEo3rWp48uXRc+ZqfP7b0sNQmrc4lRY2Xjitfk+v3PvjggxtuvfXW2MSJEwdu27bNAGbMmLHmyCOPrE/db+7cud1vuOGGg0pKSrxr166J++677+3DDjts+89+9rM1J5544iB3t3HjxsXPPffcj9K9z/nnn//++PHjy3v37r09H/0ilpxFRDqMaNksktPKHAisA2YQjd8Rak6SEd3ZML3FixfHRowY8X6m+9/7wuqeNzyzst+GTfWlvbp33nbpuPI15445rMMONlm8ePGBI0aMiGT7fJ2JdDTReJo/kEU6jnPHHPZhRy4arU19IiIikjUVERERyZqKiIiIZE1FREREsqYiIiIiWVMRERFpodaYCj6XU7hXVlYelIvXBRUREZEW29NU8JnIxRTuTVPQ33DDDTmb/UhFREQ6lpfv6MmvBw0nuv9R/HrQcF6+IydTwQNEo9E+5eXlR5SXlx9x9dVX9wb4+OOPi8aOHfvFww8/fGh5efkRt912W49rrrmm9+6mcJ8/f/4+w4cPHzJ48OChgwYNGvraa681+/o7T0H/rW99K9I0QeQZZ5wxoDX/raCLDUWkI3n5jp7MveIwGuqTf0BvXlfK3CsOA+DfLmrxBYhNU8GfcsopHz/33HNd77///gMWLFiwzN056qijhowbN27TypUrOx900EHba2pqVgF88MEHxQcccEDi5ptv7jNv3rw3+vbt27Dz6/7+97/vdckll6y7+OKLP/z000+toaGB5l7/wAMPTMRisX1uu+222L333vsOQNeuXXukmyCyNYR9e9w7zWy9mS1JiUXNbM1ON6pq2naFma0ysxVmdmo4WYtImzXvun6fFZAmDfVFzLuu1aeCr6mp6Xb66ad/tN9++zWWlZU1VlRUbPzHP/7RfdSoUVufe+65/S6++OJ+TzzxRLcDDjhgj7c8/PKXv/zJ9ddf3/eqq646aOXKlaXdunXz5l4fdpyCPtfCbs76I+nvsvffqTeqAjCzocBE4IjgOX8wM90rQ0Qyt3l9+infm4tnqKlPZPny5Uvvuuuud/fZZx9vbl7CI488sn7hwoVLhw8fvvWqq67q96Mf/WiX/oq77757/8GDBw8dPHjw0GeffbbrlClTPpw9e/aqLl26NI4fP37QnDlzuu9u3sPUKehzLdQi4u57c5e9CcAD7l7v7m+TvE3u6JwlJyLtT7fe6ad8by7eAieffPLmxx9/fP9NmzYVffzxx0WPP/54j5NOOmlTLBbr1L1798ZLLrnkw8suu2zdokWLusKOU7ifd955HzUVpRNOOGHL0qVLS4cMGVI/ffr09aeccspHixYt6tLc66fLpaSkxOvr6621/41QuH0i08zsPGA+cLm7byR5L+oXUvapRfenFpG9ceJP1uzQJwJQ0rmRE3/S6lPBH3/88VvOPvvsD0aNGjUE4Nvf/vaG4447buvDDz+83xVXXNG/qKiIkpIS/8Mf/rAadj+F+z333NPzoYceOqCkpMR79eq1/Ze//OXaPn36JNK9/ooVK3Y5qzrnnHM2DBkyZOiwYcO2zJkz5+2dt7dE6FPBm1kEeMzdhwXrfYD3AQd+DvR19wvN7CbgeXe/N9jvDuBxd384zWtOBiYHqzPdfWbO/yEiOaap4NPb26ngefmOnsy7rh+b15fSrfc2TvzJmtboVG+r2t1U8O6+rmnZzG4DHgtWa4FDUnbtD6xt5jVmAiocIrKrf7vow52LhrvzxrrNQ0uKbdsXenVbFVZqbVHYHeu7MLPUTqazgKaRW3OAiWbW2cwGAOXAS/nOT0Tan/Wb6vt0LinaGnYebVGoZyJm9tld9sysFpgBjDWzkSSbs2LAfwC4++tm9iCwFGgAprr7HofGiUi719jY2GhFRUVZtc3XNyQ6bfq0oax39851GzbX92nt5ApZY2OjAS0ayRVqEXH3dHfZa/ZWre7+C+AXuctIRNqgJRs2bBjaq1eveDaFZO1Hnx7St2yf2kSjd6hLBhobG23Dhg1lfN7ak5WC6xMREdkbDQ0N333vvfduf++994axl0309Qnvsq3BO33cuajrtoTvs2W7l25eW3RgjlItNI3AkoaGhu+25EVCH50lIpnR6KzWF6ms/iXwbZJN5PsA+wGPxKoqzg01sTZERUSkjVARya1IZfVY4Eexqoqvh51LW1Jwo7NERKTt0JmISBuhMxEpROpYF5EOJVJZfTZwLXAo8A5wZayq4v5ws2q7dCYi0kboTKTlggJyG9A1JbwF+N4uhSRatkuxIRrfeZ8YsAlIAA1E40fnKvdCpTMREelIrmXHAkKwfi3weYFIFpDUYnMYcBvRMnYpJHAS0Xjmc3e1M+pYF5GO5NAM47srNpJCRUREOpJ3MoxnWmwceJJo2QKiZZPTPaG9UxERkY7kSpJ9IKm2BPFUmRab44jGRwHjgalEy05oeYpti4qIiHQYQef594DVJM8iVpOuUz3TYhONrw1+rgf+Qge826pGZ4m0ERqdlWd7Gp0VLdsXKCIa3xQsPwVcTTT+RBjphkVFRKSNUBEpMNGygSTPPiA50vV+ovEON8u4iohIG6EiIoVIfSIiIpK1UIuImd1pZuvNbElKrKeZPWVmK4OfPYK4mdkNZrbKzF41s1HhZS4iIhD+mcgfgdN2ilUCz7h7OfBMsA7JIXTlwWMycHOechQRkWaEWkTc/Vngw53CE4C7guW7gDNT4nd70gvA/mbWNz+ZiohIOmGfiaTTx93rAIKfvYN4P+DdlP1qg5iIiISkEItIcyxNLO3QMjObbGbzg0eHnIpARCQfCnEW33Vm1tfd64LmqvVBvBY4JGW//sDadC/g7jOBmblNU0RECvFMZA5wfrB8PjA7JX5eMEprDBBvavYSEZFwhHomYmazgLHAgWZWC8wAqoAHzewiklMNfCPY/XHgdGAVyTlsvpP3hEVEZAe6Yl2kjdAV61KICrE5S0RE2ggVERERyZqKiIiIZE1FREREsqYiIiIiWVMRERGRrKmIiIhI1lREREQkayoiIiKSNRURERHJmoqIiIhkTUVERESypiIiIiJZUxEREZGsFeKdDUVEWl2ksvpw4E8poYHAz2JVFb8NKaV2QUVERDqEWFXFCmAkQKSyuhhYA/wl1KTagYItImYWAzYBCaDB3Y82s54k/5KIADHgm+6+MawcRaTNGge8GauqWB12Im1dofeJnOTuI9396GC9EnjG3cuBZ4J1EZG9NRGYFXYS7UGhF5GdTQDuCpbvAs4MMRcRaYMildWlwBnAQ2Hn0h4UchFx4EkzW2Bmk4NYH3evAwh+9k73RDObbGbzg8fkdPuISIc1HlgYq6pYF3Yi7UHB9okAx7n7WjPrDTxlZsszfaK7zwRm5i41EWnDJqGmrFZTsGci7r42+Lme5AiK0cA6M+sLEPxcH16GItLWRCqruwJfAx4JO5f2wtw97Bx2YWb7AkXuvilYfgq4muSIig/cvcrMKoGe7v5/wsxVJF/MrAbA3ceGm4nI5wq1OasP8Bczg2SO97v7E2b2MvCgmV0EvAN8I8QcRSQs0bKzgWuBQ0l+F1xJNH5/mv2KgfnAGqLxr+c1xw6iIM9ERAQildWHAHcDBwGNH/2/+0vj/7x/TYc/E0kWkNuArinRLcD3di4kT/30pHsaKD6rlIZ9L9r+49XAlbGqil2LjWStYPtERIQG4PJYVcUQYEy34V/rV3pQedc9PakDuJYdCwjB+rWpgUlX/mpaNz49++7EKfsGocOA2yKV1WfnIccOQ0VEpEDFqirqYlUVC4PlTQ0f1W0pKevdOey8CsChmcQvKn78ul80nF3kWGp4l2IjLaMiItIGRCqrI50OOKTbp6sXfxx2LgXgnT3Go2Vfr/MDui7xgen2a64ISRYKtWNdRAKRyupuwMMb5929qvHTzYmw8ykAV5K+T+TKlPXjTi2enzipeFFxZ7bTja38d6eb+M/tU6H5IiRZ0JmISAGLVFZ3Ah4G7vvktafeDzufgpDsPP8esJrkzBar2blTPRq/YnT9H847vv6GLd/f/n3+1XhEUwHZudhIC2l0lkiBilRWG8k54j6MVVVcputE9l6ksvrsrxS9+psLiuf20eis3FARESlQkcrq44HngNeAxu0frvniR/+8/61PltYcGXJqIp9RERFpI3QmIoVIfSIiBWTW9Ak31s2INDTOKPO6GZGGWdMn3Bh2TiK7ozMRkQIxa/qEG88s/tfULrbts9hWL+XRxLE3Tbpm9jQzq+naiaJPrtxvH6AzydGVfyYanxFWziI6ExEpEGOLF09JLSAAXWwbY4sXT2la37qdRuBkovERJO8XfhrRsjH5zVTkcxldJxLc23xnm9x9eyvnI9Jh9WFj8Z7iDhCNbw5WOwUPNSdIaDK92HAhcAiwETBgf6DOzNYD33P3BTnKT6TDWEePRN80hSQZT5GcmXYB8EXgJqLxF/OUosguMm3OegI43d0PdPcDSN5e8kHgEuAPuUpOpCOpSYy4ZauX7hDb6qXUJEbcskMwGk8QjY8E+gOjiZYNy1+WIjvKtIgc7e5zm1bc/UngBHd/gWQHn4i00KRrZk97NHHsTXXeI9HoUOc9Ek2d6mmfEI1/BNQAp+UzT5FUGY3OMrMngWeAB4LQt0jeYvI04GV3H5WzDHfN5TTgd0AxcLu7V+XrvUXCZGY1/bpbp9ofdq8gGv+IaFkX4EngOqLxx8LOT3YvUlkdAzYBCaAhVlVxdLgZtY5M+0TOBmYAj5LsE/lnECsGvpmb1HZlZsXATSQLWC3wspnNcfel+cpBJEyHlhWVAv8I+kWKgAdVQNqUk2JVFe1qDrSMioi7vw98v5nNq1ovnT0aDaxy97cAzOwBYAKgIiIdwvO1iU+IxseGnYdIk0yH+PYC/g9wBLBPU9zdT85RXs3pB7ybsl4LHLO7JzRNFSHSDowE/U63Vf2m3t3H67e83e97t7D5tWfWfvzin+vCzqk5ezO1TqbNWfcBfwK+DkwBzgc27HVmLWdpYrt06pjZZGBysNodKNgPS0Q6hnX3/viVhvi6bcXdD+zUZ+K1I7a/v3rL1jdfjoedV0tl2rG+wN2PMrNX3f3IIDbP3U/MeYY75vFlIOrupwbrVwC4+y/zmYdIGDQBY/sRqayOAptjVRW/DjuXlsp0iG/Tlel1ZlZhZl8iOUY9314Gys1sgJmVAhOBOSHkISKSsUhl9b6RyuruTcvAKcCScLNqHZk2Z11jZmXA5cDvgf2A/8xZVs1w9wYzmwbMJTky7E53fz3feYiI7KU+wF8ildWQ/N69P1ZV8US4KbUOzeIr0kaoOUsKUaajswaQHOIbSX2Ou5+Rm7RERNq+WdMn3Di2ePGUPmwsXkePRE1ixC3NzkDQRmXasb4YuIPgNp1NcXefl7vURCSVzkTalj3dHwaAaNn+wO3AMJIjTS8kGn8+jHyzlWmfyKfufkNOMxERaUf2cH+YprOR3wFPEI3/b6JlpUDXPKfZYpkWkd+Z2QyS8/TUNwXdfWFOshIRaeP2eH+YaNl+wAnABcn1+DZgW7rnFLJMi8hw4NvAyXzenOXBuoiI7CSD+8MMJHnR9v8QLRtB8h4xPyAa/ySfebZUpteJnAUMdPcT3f2k4KECIiLSjAzuD1MCjAJuJhr/EvAJUJnfLFsu0yKymOTdDEVEJAMZ3B+mFqhNuTPln0kWlTYl09FZNcCRJK8YT+0T0RBfkTzR6Kx2KFr2HPBdovEVRMuiwL5E4z8OOau9kmmfyIycZiEi0jF9H7gvGJn1FvCdkPPZa7piXaSN0JmIFKLdnomY2SbSTLVOckp2d/f9cpKViIi0CbstIu7ePV+JiIhI25Pp6CwREZFdqIiIiEjWVERERCRrKiIiIpK1gisiZhY1szVmtih4nJ6y7QozW2VmK8zs1DDzFBGRzC82zLf/dvcdbmBvZkNJ3lP9COBg4GkzG+TuiTASFBGRAjwT2Y0JwAPuXu/ubwOrgNEh5yQi0qEVahGZZmavmtmdZtYjiPUD3k3ZpzaIiYhISEIpImb2tJktSfOYANwMfAEYCdQB1zc9Lc1LpZ2zxcwmm9n84DE5J/8IEREJp0/E3b+ayX5mdhvwWLBaCxySsrk/sLaZ158JzGxJjiIismcF15xlZn1TVs8ClgTLc4CJZtbZzAYA5cBL+c5PREQ+V4ijs35lZiNJNlXFgP8AcPfXzexBYCnQAEzVyCwRkXBpKniRNkJTwUshKrjmLBERaTtUREREJGsqIiIikjUVERERyZqKiIiIZE1FREREsqYiIiIiWSvEiw0LVqSyen/gdmAYyYshL4xVVTwfblYiIuHRmcje+R3wRKyqYjAwAlgWcj4iIqHSmUiGIpXV+wEnABcAxKoqtgHbwsxJRCRsKiKZGwhsAP4nUlk9AlgA/CBWVfFJuGmJiIRHzVmZKwFGATfHqiq+BHwCVIabkohIuFREMlcL1MaqKl4M1v9MsqiIiHRYKiIZilVVvAe8G6msPjwIjSM5Lb2ISIelPpG9833gvkhldSnwFvCdkPMREQmV7ici0kbofiJSiEJpzjKzb5jZ62bWaGZH77TtCjNbZWYrzOzUlPhpQWyVmeWtQ3vW9Ak31s2INDTOKPO6GZGGWdMn3Jiv9xYRKXShnImY2RCgEbgV+JG7zw/iQ4FZwGjgYOBpYFDwtDeAr5Hs4H4ZmOTuOe2TmDV9wo1nFv9rahf7/HKQrV7Ko4ljb5p0zexpAETLTiN5EWIxcDvReFUuc5KOS2ciUohCORNx92XuviLNpgnAA+5e7+5vA6tIFpTRwCp3f8vdtwEPBPvm1NjixVNSCwhAF9vG2OLFUwCIlhUDNwHjgaHAJKJlQ3Odl4hIoSi00Vn9gHdT1muDWHPxtMxsspnNDx6Ts02mDxuL9xAfDawiGn+LaDxvxU1EpFDkbHSWmT0NHJRm01XuPru5p6WJOemLXbPtcO4+E5i5xyT3YB09En3TFJJkHEhf3I5p6fuKiLQVOSsi7v7VLJ5WCxySst4fWBssNxfPmZrEiFvS9YnUJEbcMim52lzRExHpEAqtOWsOMNHMOpvZAKAceIlkR3q5mQ0ws1JgYrBvTk26Zva0RxPH3lTnPRKNDnXeI7FDp/rui56ISLsX1uiss4DfA72Aj4BF7n5qsO0q4EKgAbjM3f8WxE8HfktyFNSd7v6LvCe+s2hZCclRY+OANSSL3dlE46+Hmpe0SxqdJYVIFxu2VLRsh+JGNB5+cZN2SUVECpGKiEgboSIihajQ+kRERKQNUREREZGsqYiIiEjWVERERCRrKiIiIpI1FREREcmaioiIiGRNRURERLKmIiIiIllTERERkaypiIiISNZUREREJGsqIiIikjUVERERyVooRcTMvmFmr5tZo5kdnRKPmNlWM1sUPG5J2XaUmb1mZqvM7AYzS3drWhERyaOwzkSWAP8OPJtm25vuPjJ4TEmJ3wxMJnnL3HLgtNynKSIiuxNKEXH3Ze6+ItP9zawvsJ+7P+/Ju2jdDZyZswRFRCQjhdgnMsDMXjGzeWb2lSDWD6hN2ac2iImISIhKcvXCZvY0cFCaTVe5++xmnlYHHOruH5jZUcCjZnYEkK7/o9n7+prZZJJNXwAz3X3mXqQuIiIZylkRcfevZvGceqA+WF5gZm8Cg0ieefRP2bU/sHY3rzMTUOEQEcmxgmrOMrNeZlYcLA8k2YH+lrvXAZvMbEwwKus8oLmzGRERyZOwhvieZWa1wJeBajObG2w6AXjVzBYDfwamuPuHwbaLgduBVcCbwN/ynLaIiOzEkoOdRKTQmVkNgLuPDTcTkc8VVHOWiIi0LSoiIiKSNRURERHJmoqIiIhkTUVERESypiIiIiJZy9kV6yIikn+Ryuo7ga8D62NVFcNy/X46ExERaV/+SB5vlaEiIiLSjsSqKp4FPtzjjq1ERURERLKmIiIiIllTERERkaypiIiISNZURERE2pFIZfUs4Hng8EhldW2ksvqiXL6fpoIXaSM0FbwUIp2JiIhI1kK5Yt3M/gv4X8A2kncp/I67fxRsuwK4CEgAl7r73CB+GvA7oBi43d2rwshdRKTQzJo+4caxxYun9GFj8Tp6JGoSI26ZdM3saZ/tEC07HPhTylMGAj8jGv9tS99RsbrLAAAH+ElEQVQ7lOYsMzsF+Lu7N5jZdQDu/hMzGwrMAkYDBwNPA4OCp70BfA2oBV4GJrn70rwnLxISNWdJOrOmT7jxzOJ/Te1i2z6LbfVSHk0ce9MOhaRJtKwYWAMcQzS+uqXvH0pzlrs/6e4NweoLQP9geQLwgLvXu/vbJO+nPjp4rHL3t9x9G/BAsK+ISIc2tnjxlNQCAtDFtjG2ePGUZp4yDnizNQoIFEafyIXA34LlfsC7Kdtqg1hz8bTMbLKZzQ8ek1s5XxGRgtGHjcV7EwcmkmzxaRU56xMxs6eBg9JsusrdZwf7XAU0APc1PS3N/k76YtdsO5y7zwRm7lXCIiJt0Dp6JPqmKRjJ+E6iZaXAGcAVrfX+OSsi7v7V3W03s/NJTlc8zj/vmKkFDknZrT+wNlhuLi4i0mHVJEbckq5PpCYx4pZJu+4+HlhINL6utd4/lOasYKTVT4Az3H1LyqY5wEQz62xmA4By4CWSHenlZjbAzEpJno7NyXfeIiKFZtI1s6c9mjj2pjrvkWh0qPMeiWY71WESrdiUBeGNzloFdAY+CEIvuPuUYNtVJPtJGoDL3P1vQfx04Lckh/je6e6/yHviIiHS6CxpkWhZV5J9ywOJxuOt9bK6Yl2kjVARkUJUCKOzRESkjVIRERGRrKmIiIhI1lREREQkayoiIiKSNRURERHJmoqIiIhkTUWkhQp5gkfllp1CzS24PuT+sPNoTqEeN1BuuaQi0nKF/Aug3LKj3LKj3LJTyLntkYqIiIhkTUVERESypiLScoV83xLllh3llh3llp1Czm2PNAGjiIhkTWciIiKSNRWRDJnZN8zsdTNrNLOjd9p2hZmtMrMVZnZqSvy0ILbKzCrzmOufzGxR8IiZ2aIgHjGzrSnbbslXTim5Rc1sTUoOp6dsS3sc85jbf5nZcjN71cz+Ymb7B/HQj1uQRyi/T83kcoiZ/cPMlgX/L34QxJv9fPOcX8zMXgtymB/EeprZU2a2MvjZI4S8Dk85NovM7GMzu6xQjltW3F2PDB7AEOBwoAY4OiU+FFhM8iZbA4A3Sd44qzhYHgiUBvsMDSHv64GfBcsRYEnIxzEK/ChNPO1xzHNupwAlwfJ1wHUFdNwK4vcpJZ++wKhguTvwRvAZpv18Q8gvBhy4U+xXQGWwXNn0+Yb8mb4HHFYoxy2bh85EMuTuy9x9RZpNE4AH3L3e3d8GVgGjg8cqd3/L3bcBDwT75o2ZGfBNWvl2mDnS3HHMG3d/0t0bgtUXgP75fP89CP33KZW717n7wmB5E7AM6BdWPhmaANwVLN8FnBliLgDjgDfdfXXIebSIikjL9SN5y8kmtUGsuXg+fQVY5+4rU2IDzOwVM5tnZl/Jcz5NpgVNRnemNCkUwvFKdSHwt5T1sI9boR2fz5hZBPgS8GIQSvf55psDT5rZgpQrwvu4ex0kiyDQO6Tcmkxkxz/wCuG47TUVkRRm9rSZLUnz2N1ffJYm5ruJt4oMc53Ejr+kdcCh7v4l4IfA/Wa2X2vllGFuNwNfAEYG+Vzf9LQ0L9XqQwczOW5mdhXQANwXhPJy3PaUeppY6EMrzawb8DBwmbt/TPOfb74d5+6jgPHAVDM7IaQ80jKzUuAM4KEgVCjHba+VhJ1AIXH3r2bxtFrgkJT1/sDaYLm5eIvtKVczKwH+HTgq5Tn1QH2wvMDM3gQGAfNbK69MckvJ8TbgsWB1d8ex1WRw3M4Hvg6M86DhOl/HbQ/ycnz2hpl1IllA7nP3RwDcfV3K9tTPN6/cfW3wc72Z/YVkc+A6M+vr7nVm1hdYH0ZugfHAwqbjVSjHLRs6E2m5OcBEM+tsZgOAcuAl4GWg3MwGBH91TAz2zZevAsvdvbYpYGa9zKw4WB4Y5PpWHnMi+M/b5CxgSbDc3HHMZ26nAT8BznD3LSnx0I8b4f8+7SDob7sDWObuv0mJN/f55jO3fc2se9MyyQETS0ger/OD3c4HZuc7txQ7tBIUwnHLls5EMmRmZwG/B3oB1Wa2yN1PdffXzexBYCnJJpCp7p4InjMNmEtyFMad7v56HlPeub0V4ATgajNrABLAFHf/MI85AfzKzEaSbIqJAf8BsLvjmEc3khwd9lTyO5IX3H0KBXDc3L0h5N+nnR0HfBt4zYIh5MCVwKR0n2+e9QH+EnyGJcD97v6Emb0MPGhmFwHvAN8IITfMrCvwNXY8Nmn/X7QFumJdRESypuYsERHJmoqIiIhkTUVERESypiIiIiJZUxEREZGsqYiItDIzSwQzsS4xs79aMBtwlq8VM7MDWzM/kdakIiLS+ra6+0h3HwZ8CEwNOyGRXFEREcmt50mZKNHMfmxmLwcT7f3flPijwWSBr6dMGChS8FRERHIkmCplHMH0JGZ2CskpU0aTnGjvqJSJAS9096OAo4FLzeyAEFIW2WsqIiKtr0swFcgHQE/gqSB+SvB4BVgIDCZZVCBZOBaTvI/JISlxkYKmIiLS+ra6+0iSd6wr5fM+EQN+GfSXjHT3L7r7HWY2luSEmV929xEki8w+YSQusrdURERyxN3jwKXAj4Jp0+cCFwb34MDM+plZb6AM2OjuW8xsMDAmtKRF9pJm8RXJIXd/JWimmuju95jZEOD5YIbZzcC5wBPAFDN7FVhBsklLpE3QLL4iIpI1NWeJiEjWVERERCRrKiIiIpI1FREREcmaioiIiGRNRURERLKmIiIiIllTERERkaz9fy3acPf14ZjoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test = np.random.randint(-200,200,8) + 1j * np.random.randint(-200, 200, 8)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def label_complex(zs, color, xyoffset, label):\n",
    "    ax.scatter(np.real(zs), np.imag(zs), c=color, label=label)    \n",
    "    for i, z in enumerate(zs):\n",
    "        ax.annotate(str(i), (np.real(z), np.imag(z)), color=color, textcoords=\"offset points\", xytext=xyoffset)\n",
    "\n",
    "        \n",
    "label_complex(test, 'C0', (0,5), label=\"Pre-sort\")\n",
    "\n",
    "smooth_sorted = diff_sort(matrices, test, softmax=smoothmax)\n",
    "label_complex(smooth_sorted, 'C1', (5, -5), \"Post-sort\")\n",
    "ax.axhline(0, c='k')\n",
    "ax.axvline(0, c='k')\n",
    "ax.set_xlabel(\"Real\")\n",
    "ax.set_ylabel(\"Imag\")\n",
    "ax.set_frame_on(False)\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relaxed sorting\n",
    "We can define a slighly modified function which interpolates between `softmax(a,b)` and `mean(a,b)`. The result is a sorting function that can be relaxed from sorting to averaging. The $\\alpha$ parameter of `smoothmax` also allows relaxation to the mean, though with different scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact sorting           -102.00\t-91.00\t-64.00\t-15.00\t-12.00\t 31.00\t139.00\t151.00\n",
      "\n",
      "Softmax.   smooth=0.00  -102.00\t-91.00\t-64.00\t-15.05\t-11.95\t 31.00\t139.00\t151.00\n",
      "Smoothmax. α=1.00       -102.00\t-91.00\t-64.00\t-14.86\t-12.14\t 31.00\t139.00\t151.00\n",
      "\n",
      "Softmax.   smooth=0.25  -45.91\t-42.19\t-26.17\t -7.11\t 16.03\t 20.25\t 58.97\t 63.14\n",
      "Smoothmax. α=0.75       -102.00\t-91.00\t-64.00\t-14.71\t-12.29\t 31.00\t139.00\t151.00\n",
      "\n",
      "Softmax.   smooth=0.50  -17.52\t-15.16\t -1.76\t  0.95\t 12.55\t 14.70\t 21.22\t 22.01\n",
      "Smoothmax. α=0.50       -101.96\t-91.04\t-64.00\t-14.45\t-12.55\t 31.00\t139.03\t150.97\n",
      "\n",
      "Softmax.   smooth=0.75   -0.63\t -0.35\t  3.02\t  3.29\t  7.11\t  7.46\t  8.39\t  8.71\n",
      "Smoothmax. α=0.25       -101.34\t-91.63\t-64.04\t-14.04\t-12.96\t 31.00\t139.57\t150.43\n",
      "\n",
      "Softmax.   smooth=1.00    4.62\t  4.62\t  4.62\t  4.62\t  4.62\t  4.62\t  4.62\t  4.62\n",
      "Smoothmax. α=0.00         4.62\t  4.62\t  4.62\t  4.62\t  4.62\t  4.62\t  4.62\t  4.62\n",
      "\n",
      "Mean 4.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Differentiable smoothed sorting \n",
    "test = np.random.randint(-200,200,8)\n",
    "\n",
    "print(\"Exact sorting          \", neat_vec(diff_sort(matrices, test, np.maximum)))\n",
    "print()\n",
    "\n",
    "for smooth in np.linspace(0, 1, 5):    \n",
    "    print(f\"Softmax.   smooth={smooth:.2f} \", neat_vec(diff_sort(matrices, test, lambda a,b:softmax_smooth(a,b,smooth=smooth))))\n",
    "    # smoothmax's alpha is the inverse of diff_bisort_smooth\n",
    "    print(f\"Smoothmax. α={1-smooth:.2f}      \", neat_vec(diff_sort(matrices, test, lambda a,b:smoothmax(a,b, alpha=1-smooth))))\n",
    "    print()\n",
    "    \n",
    "print(f\"Mean {np.mean(test):.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of the sorting process\n",
    "Now we can take the derivatives of the sorting networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.02, 0.  , 0.02, 0.86, 0.05, 0.05],\n",
       "       [0.02, 0.  , 0.  , 0.02, 0.02, 0.04, 0.86, 0.02],\n",
       "       [0.02, 0.02, 0.  , 0.  , 0.02, 0.04, 0.02, 0.86],\n",
       "       [0.86, 0.04, 0.02, 0.  , 0.02, 0.  , 0.02, 0.02],\n",
       "       [0.04, 0.86, 0.02, 0.05, 0.  , 0.  , 0.  , 0.02],\n",
       "       [0.  , 0.04, 0.04, 0.86, 0.02, 0.  , 0.02, 0.  ],\n",
       "       [0.02, 0.  , 0.02, 0.02, 0.86, 0.02, 0.02, 0.02],\n",
       "       [0.02, 0.02, 0.86, 0.05, 0.02, 0.02, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autograd import jacobian\n",
    "\n",
    "matrices = bitonic_matrices(8)\n",
    "test = np.random.randint(-200,200,8)\n",
    "# show that we can take the derivative\n",
    "jac_sort = jacobian(diff_sort, argnum=1)\n",
    "jac_sort(matrices, test, softmax=lambda a,b:softmax_smooth(a,b,0.05)) # slight relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-168.00\t-43.00\t-30.00\t-13.00\t 41.00\t 63.00\t 92.00\t164.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.  ,  0.  ,  0.  , -0.  ,  0.  ,  1.  , -0.  , -0.  ],\n",
       "       [-0.  ,  0.  , -0.  ,  0.  , -0.  , -0.  ,  1.01, -0.01],\n",
       "       [-0.  ,  0.  ,  0.  , -0.  ,  0.  , -0.  , -0.01,  1.01],\n",
       "       [ 1.  , -0.  , -0.  ,  0.  , -0.  ,  0.  ,  0.  , -0.  ],\n",
       "       [-0.  ,  1.  , -0.  , -0.  ,  0.  ,  0.  ,  0.  , -0.  ],\n",
       "       [ 0.  , -0.  , -0.  ,  1.  , -0.  , -0.  , -0.  ,  0.  ],\n",
       "       [-0.  ,  0.  , -0.  , -0.  ,  1.  , -0.  ,  0.  , -0.  ],\n",
       "       [ 0.  , -0.  ,  1.  ,  0.  , -0.  ,  0.  , -0.  ,  0.  ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same, but using smoothmax\n",
    "print(neat_vec(diff_sort(matrices, test, smoothmax)))\n",
    "jac_sort(matrices, test,  lambda a,b:smoothmax(a,b,alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexed form\n",
    "Rather than using matrix multiplies, we can instead just generate the permutation indices and reindex the vectors (assuming the backend supports array indexing).\n",
    "\n",
    "This requires only 2n indexing operations at each layer instead of 2 n x n matrix multiplies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact sorting       -174.00\t-44.00\t  8.00\t 17.00\t 22.00\t 24.00\t131.00\t172.00\n",
      "Indexed            -174.00\t-44.00\t  8.00\t 17.04\t 22.20\t 23.76\t131.00\t172.00\n"
     ]
    }
   ],
   "source": [
    "from differentiable_sorting import bitonic_indices, diff_sort_indexed\n",
    "test = np.random.randint(-200,200,8)\n",
    "indices = bitonic_indices(8)\n",
    "\n",
    "print(\"Exact sorting      \", neat_vec(diff_sort(matrices, test, np.maximum)))\n",
    "print(f\"Indexed           \", neat_vec(diff_sort_indexed(indices, test, smoothmax)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable ranking / argsort\n",
    "We can use a differentiable similarity measure between the input (unsorted) and output (sorted) of the sorting network, e.g. using an RBF kernel. We can use this to generate a normalised similarity matrix and apply this to a vector `[1, 2, 3, ..., n]`. This gives a differentiable ranking function.\n",
    "\n",
    "As `sigma`, the RBF width goes to infinity, the result converges to giving all values the mean rank; as it goes to zero the result converges to the true rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting import order_matrix, diff_argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x               5.00\t -1.00\t  9.50\t 13.20\t 16.20\t 10.50\t 42.00\t 18.00\n",
      "diff_argsort    1.00\t  0.00\t  2.00\t  4.00\t  5.00\t  3.00\t  7.00\t  6.00\n",
      "exact argsort   1.00\t  0.00\t  2.00\t  5.00\t  3.00\t  4.00\t  7.00\t  6.00\n"
     ]
    }
   ],
   "source": [
    "x = np.array([5.0, -1.0, 9.5, 13.2, 16.2, 10.5, 42.0, 18.0])\n",
    "\n",
    "\n",
    "print(\"x            \", neat_vec(x))\n",
    "# show argsort\n",
    "ranks = diff_argsort(matrices, x, sigma=0.25)\n",
    "print(\"diff_argsort \", neat_vec(ranks))\n",
    "print(\"exact argsort\",neat_vec(np.argsort(ranks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "# we now have differentiable argmax and argmin by indexing the rank vector\n",
    "print(np.argmin(x), int(ranks[0]+0.5))\n",
    "print(np.argmax(x), int(ranks[-1]+0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing ranks\n",
    "We can again relax argsort back to the mean rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed ranks\n",
      "sigma=    0.1  |   1.00\t  0.00\t  2.00\t  4.00\t  5.00\t  3.00\t  7.00\t  6.00\n",
      "sigma=    1.0  |   1.00\t  0.00\t  2.33\t  3.97\t  5.12\t  2.73\t  7.00\t  5.85\n",
      "sigma=   10.0  |   2.55\t  1.92\t  3.01\t  3.38\t  3.65\t  3.11\t  6.79\t  3.82\n",
      "sigma=  100.0  |   3.47\t  3.45\t  3.48\t  3.49\t  3.49\t  3.48\t  3.56\t  3.50\n",
      "sigma= 1000.0  |   3.50\t  3.50\t  3.50\t  3.50\t  3.50\t  3.50\t  3.50\t  3.50\n"
     ]
    }
   ],
   "source": [
    "print(\"Smoothed ranks\")\n",
    "test = x\n",
    "for sigma in [0.1, 1, 10, 100, 1000]:     \n",
    "    ranks = diff_argsort(matrices, test, sigma=sigma) \n",
    "    print(f\"sigma={sigma:7.1f}  |\", neat_vec(ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Differentiable ranking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "jac_rank = jacobian(diff_argsort, argnum=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_argsort   0.79\t  1.49\t  2.25\t  3.21\t  6.38\t  5.67\t  5.05\t  3.21\n",
      "argsort        0.00\t  1.00\t  2.00\t  3.00\t  7.00\t  6.00\t  5.00\t  4.00\n",
      "Jacobian of diff_argsort(x)\n",
      "[[ 0.59  -0.186 -0.157 -0.12  -0.005 -0.009 -0.028 -0.084]\n",
      " [-0.131  0.506 -0.105 -0.104 -0.01  -0.019 -0.042 -0.094]\n",
      " [-0.136 -0.155  0.794 -0.171 -0.029 -0.049 -0.095 -0.159]\n",
      " [-0.102 -0.13  -0.142  0.714 -0.044 -0.069 -0.11  -0.117]\n",
      " [-0.005 -0.01  -0.029 -0.049  0.547 -0.199 -0.178 -0.077]\n",
      " [-0.009 -0.016 -0.038 -0.059 -0.139  0.453 -0.117 -0.076]\n",
      " [-0.031 -0.052 -0.096 -0.127 -0.119 -0.135  0.696 -0.136]\n",
      " [-0.102 -0.13  -0.142 -0.135 -0.044 -0.069 -0.11   0.732]]\n"
     ]
    }
   ],
   "source": [
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 8, 7, 6, 4])\n",
    "ranks = diff_argsort(matrices, x, sigma=1.0)\n",
    "print(\"diff_argsort\", neat_vec(ranks))\n",
    "print(\"argsort     \", neat_vec(np.argsort(ranks)))\n",
    "\n",
    "print(\"Jacobian of diff_argsort(x)\")\n",
    "print(jac_rank(matrices, np.array(x), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: which elements should we tweak the least to change the ranking the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x          41.00\t  2.00\t 30.00\t 40.00\t 50.00\t 60.00\t 70.00\t190.00\n",
      "d_rank/dx  -0.68\t -0.00\t -0.00\t  0.68\t -0.00\t -0.00\t -0.00\t  0.00\n"
     ]
    }
   ],
   "source": [
    "matrices = bitonic_matrices(8)\n",
    "from autograd import grad\n",
    "\n",
    "# which elements cause the biggest change in ranking if adjusted?\n",
    "# we can compute this directly:\n",
    "# obviously 41 and 40 are vying for position so we should see\n",
    "# large changes in rank for a small change in their value\n",
    "x = np.array([41, 2, 30, 40, 50, 60, 70, 190])\n",
    "\n",
    "# approximate change in rank as first moment of ranks\n",
    "rank_change = lambda x: np.sum((diff_argsort(matrices, x, sigma=1.0)* np.arange(1,9)))\n",
    "grad_rank_change = grad(rank_change)\n",
    "print(\"x        \", neat_vec(x))\n",
    "print(\"d_rank/dx\", neat_vec(grad_rank_change(np.array(x))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting.torch import softmax, diff_argsort, diff_sort, bitonic_matrices\n",
    "\n",
    "torch_matrices = bitonic_matrices(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.3723e-03, 4.6156e-01, 4.7606e-04,  ..., 2.6223e-01, 9.4799e-03,\n",
      "         6.2101e-06],\n",
      "        [2.6141e-03, 2.6865e-01, 3.7196e-04,  ..., 4.5689e-01, 1.4254e-02,\n",
      "         9.4818e-06],\n",
      "        [8.1700e-03, 8.8367e-02, 1.2905e-03,  ..., 9.1891e-02, 2.7098e-02,\n",
      "         1.9586e-05],\n",
      "        ...,\n",
      "        [5.3572e-04, 1.2904e-06, 2.5379e-03,  ..., 1.9927e-06, 1.6432e-04,\n",
      "         1.4978e-01],\n",
      "        [3.0070e-04, 7.9951e-07, 1.2954e-03,  ..., 5.7468e-07, 4.8339e-05,\n",
      "         5.0351e-02],\n",
      "        [2.4999e-05, 6.4037e-08, 1.1568e-04,  ..., 1.0350e-07, 8.4775e-06,\n",
      "         7.2719e-03]])\n"
     ]
    }
   ],
   "source": [
    "test_input = np.random.normal(0, 5, 32)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device),\n",
    "                          requires_grad=True)\n",
    "\n",
    "result = diff_sort(torch_matrices, var_test_input, softmax=softmax)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the\n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(\n",
    "        torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 32 x 32 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.0000,  0.9999, 18.5322,  3.0009,  5.0970,  7.0002, 15.0003,  5.0028,\n",
      "        20.0000, 15.9881, 15.8892, 25.1668, 27.8087, 15.0352, 19.0000, 29.0000,\n",
      "        19.0000, 12.0000,  9.2568,  3.8031,  7.9369, 28.0000, 24.0000, 31.0000,\n",
      "        26.0927, 23.0157, 20.0000, 11.0002, 11.0000,  1.0000,  7.0000, 27.9528],\n",
      "       grad_fn=<MvBackward>)\n"
     ]
    }
   ],
   "source": [
    "result = diff_argsort(torch_matrices, var_test_input, transpose=False)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
