{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations executed in parallel. A sorting network implements  a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$ (sorting networks for non-power-of-2 sizes are possible but not trickier).\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network for $n=2^k$ elements has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a $k$ element vector into sorted order.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification!) as the continuous approximation to the `max(a,b)` function:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b) \\approx \\max(a,b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin(a,b)` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b}) \\approx \\min(a,b).$$ More numerically stably we can write: \n",
    "\n",
    "$$\\text{softmin}(a,b) = a + b - \\text{softmax}(a,b).$$\n",
    "\n",
    "These functions obviously aren't equal to max and min, but are relatively close, and differentiable. Note that we now have a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{high} = \\text{softmax}(a,b), \\text{low} = \\text{softmin}(a,b), \\text{where } \\text{low}\\leq \\text{high}$$\n",
    "\n",
    "Alternatively, we can use: \n",
    "$$\\text{smoothmax}(a,b) = \\frac{a (e^{\\alpha a}) + b (e^{\\alpha b})}{e^{\\alpha a}+e^{\\alpha b}}  \\approx \\max(a,b).$$  This has an adjustable smoothness parameter $\\alpha$, with exact maximum as $\\alpha \\rightarrow \\infty$ and pure averaging as $\\alpha \\rightarrow 0$.\n",
    "\n",
    "## Differentiable sorting\n",
    "\n",
    "For each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides which can be done simultaneously. We can any write function that selects the relevant elements of the vector as a multiply with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ and $R \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{k}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ and $R' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i[\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i})] + R'_i[\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})]$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is clearly differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network. \n",
    "\n",
    "This process is excessively computation heavy, but easy to compute. We could also simplify this into two matrix multiplies, at the cost of a vector split and join in the middle (see the `woven` form later in this text). \n",
    "\n",
    "## Example\n",
    "\n",
    "To sort four elements, we have a network like:\n",
    "\n",
    "    0  1  2  3  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕<<┙  \n",
    "    ┕>>>>>┙  │  \n",
    "    │  │  │  │  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕>>┙  \n",
    "    \n",
    "This is equivalent to: \n",
    "\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[3], x[2] = cswap(x[2], x[3])\n",
    "    x[0], x[2] = cswap(x[0], x[2])\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[2], x[3] = cswap(x[2], x[3])\n",
    "    \n",
    "where `cswap(a,b) = (min(a,b), max(a,b))`\n",
    "\n",
    "Replacing the indexing with matrix multiplies and `cswap` with a `softcswap = (softmin(a,b), softmax(a,b))` we then have the differentiable form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0>1\t 2<3\t 4>5\t 6<7\t 8>9\t10<11\t12>13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>2\t 1>3\t 4<6\t 5<7\t 8>10\t 9>11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4<5\t 6<7\t 8>9\t10>11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8<12\t 9<13\t10<14\t11<15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8<10\t 9<11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8<9\t10<11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>8\t 1>9\t 2>10\t 3>11\t 4>12\t 5>13\t 6>14\t 7>15\t\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8>12\t 9>13\t10>14\t11>15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8>10\t 9>11\t12>14\t13>15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8>9\t10>11\t12>13\t14>15\t\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from bitonic_tests import bitonic_network, pretty_bitonic_network\n",
    "\n",
    "def neat_vec(n):\n",
    "    # print a vector neatly    \n",
    "    return \"\\t\".join([f\"{x:6.2f}\" for x in n])\n",
    "\n",
    "# this should match the diagram at the top of the notebook\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  1  2  3  4  5  6  7 \n",
      " ╭──╯  │  │  │  │  │  │ \n",
      " │  │  ╰──╮  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │ \n",
      " │  │  │  │  │  │  ╰──╮ \n",
      " ╭─────╯  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │ \n",
      " │  │  │  │  ╰─────╮  │ \n",
      " │  │  │  │  │  ╰─────╮ \n",
      " ╭──╯  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │ \n",
      " │  │  │  │  ╰──╮  │  │ \n",
      " │  │  │  │  │  │  ╰──╮ \n",
      " ╭───────────╯  │  │  │ \n",
      " │  ╭───────────╯  │  │ \n",
      " │  │  ╭───────────╯  │ \n",
      " │  │  │  ╭───────────╯ \n",
      " ╭─────╯  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │ \n",
      " │  │  │  │  ╭─────╯  │ \n",
      " │  │  │  │  │  ╭─────╯ \n",
      " ╭──╯  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │ \n",
      " │  │  │  │  │  │  ╭──╯ \n"
     ]
    }
   ],
   "source": [
    "pretty_bitonic_network(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing sorting network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24.00\t 45.00\t104.00\t123.00\t149.00\t163.00\t180.00\t183.00\n",
      " 16.00\t 16.00\t 18.00\t 46.00\t 75.00\t 79.00\t145.00\t193.00\n",
      " 16.00\t 28.00\t 39.00\t 68.00\t 75.00\t 90.00\t163.00\t179.00\n",
      " 14.00\t 21.00\t 49.00\t 65.00\t 76.00\t 84.00\t102.00\t166.00\n",
      " 10.00\t 51.00\t 88.00\t 97.00\t 99.00\t124.00\t134.00\t141.00\n"
     ]
    }
   ],
   "source": [
    "# Test sorting\n",
    "import autograd.numpy as np # we can use plain numpy as well (but can't take grad!)\n",
    "\n",
    "\n",
    "from differentiable_sorting import bitonic_matrices, diff_sort, diff_argsort\n",
    "from differentiable_sorting import softmax, smoothmax, softmax_smooth\n",
    "\n",
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "\n",
    "# test bitonic sorting with exact maximum; this should work exactly as regular sorting\n",
    "for i in range(5):\n",
    "    # these should all be in sorted order\n",
    "    test = np.random.randint(0, 200, 8)\n",
    "    print(neat_vec(diff_sort(matrices, test, softmax=np.maximum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax sorting    -171.00\t-143.00\t-106.00\t-27.31\t-25.69\t  7.00\t 58.00\t 68.00\n",
      "Smoothmax sorting  -171.00\t-143.00\t-106.00\t-26.73\t-26.27\t  7.00\t 58.00\t 68.00\n",
      "Exact sorting      -171.00\t-143.00\t-106.00\t-27.00\t-26.00\t  7.00\t 58.00\t 68.00\n",
      "\n",
      "Softmax sorting    -169.00\t-79.00\t 41.00\t 82.00\t104.00\t140.00\t158.00\t165.00\n",
      "Smoothmax sorting  -169.00\t-79.00\t 41.00\t 82.00\t104.00\t140.00\t158.01\t164.99\n",
      "Exact sorting      -169.00\t-79.00\t 41.00\t 82.00\t104.00\t140.00\t158.00\t165.00\n",
      "\n",
      "Softmax sorting    -162.00\t-110.00\t-51.00\t-19.00\t 30.00\t 56.00\t 81.00\t170.00\n",
      "Smoothmax sorting  -162.00\t-110.00\t-51.00\t-19.00\t 30.00\t 56.00\t 81.00\t170.00\n",
      "Exact sorting      -162.00\t-110.00\t-51.00\t-19.00\t 30.00\t 56.00\t 81.00\t170.00\n",
      "\n",
      "Softmax sorting    -156.00\t-112.01\t-106.99\t-96.00\t-55.01\t-49.99\t-16.00\t  7.00\n",
      "Smoothmax sorting  -156.00\t-111.97\t-107.03\t-96.00\t-54.97\t-50.03\t-16.00\t  7.00\n",
      "Exact sorting      -156.00\t-112.00\t-107.00\t-96.00\t-55.00\t-50.00\t-16.00\t  7.00\n",
      "\n",
      "Softmax sorting    -169.00\t-151.00\t-92.00\t 10.00\t 19.00\t 53.00\t 92.00\t160.00\n",
      "Smoothmax sorting  -169.00\t-151.00\t-92.00\t 10.00\t 19.00\t 53.00\t 92.00\t160.00\n",
      "Exact sorting      -169.00\t-151.00\t-92.00\t 10.00\t 19.00\t 53.00\t 92.00\t160.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differentiable sorting \n",
    "np.set_printoptions(precision=2)\n",
    "matrices = bitonic_matrices(8) \n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    # note the range here is chosen to work well: small values will not work well! \n",
    "    test = np.random.randint(-200,200,8)\n",
    "    print(\"Softmax sorting   \", neat_vec(diff_sort(matrices, test, softmax=softmax)))\n",
    "    print(\"Smoothmax sorting \", neat_vec(diff_sort(matrices, test, softmax=smoothmax)))\n",
    "    print(\"Exact sorting     \", neat_vec(diff_sort(matrices, test, softmax=np.maximum)))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive-only sorting\n",
    "Slightly better accuracy can be achieved with normalised softmax *if* all elements are strictly positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax sorting     -0.52\t  0.91\t  1.56\t  2.96\t  5.04\t  6.44\t  7.09\t  8.52\n",
      "Posmax sorting       0.13\t  0.76\t  1.38\t  2.67\t  5.03\t  6.43\t  7.09\t  8.52\n",
      "Exact sorting        1.00\t  1.00\t  1.00\t  2.00\t  6.00\t  7.00\t  7.00\t  7.00\n",
      "\n",
      "Softmax sorting      0.80\t  3.05\t  3.85\t  5.29\t  5.19\t  6.58\t  6.93\t  8.32\n",
      "Posmax sorting       0.88\t  3.02\t  3.84\t  5.27\t  5.19\t  6.57\t  6.92\t  8.31\n",
      "Exact sorting        1.00\t  4.00\t  4.00\t  6.00\t  6.00\t  6.00\t  6.00\t  7.00\n",
      "\n",
      "Softmax sorting      0.47\t  2.38\t  2.90\t  4.28\t  4.27\t  5.67\t  6.26\t  7.77\n",
      "Posmax sorting       0.66\t  2.31\t  2.87\t  4.23\t  4.26\t  5.65\t  6.25\t  7.76\n",
      "Exact sorting        1.00\t  3.00\t  4.00\t  4.00\t  4.00\t  5.00\t  6.00\t  7.00\n",
      "\n",
      "Softmax sorting      0.09\t  1.49\t  2.13\t  3.59\t  4.10\t  5.76\t  6.72\t  8.13\n",
      "Posmax sorting       0.44\t  1.40\t  2.06\t  3.48\t  4.06\t  5.73\t  6.71\t  8.12\n",
      "Exact sorting        1.00\t  2.00\t  2.00\t  3.00\t  4.00\t  6.00\t  7.00\t  7.00\n",
      "\n",
      "Softmax sorting      0.23\t  2.06\t  2.78\t  4.23\t  4.47\t  5.88\t  6.48\t  7.88\n",
      "Posmax sorting       0.51\t  1.93\t  2.74\t  4.16\t  4.46\t  5.87\t  6.48\t  7.87\n",
      "Exact sorting        1.00\t  2.00\t  3.00\t  5.00\t  5.00\t  5.00\t  6.00\t  7.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    test = np.random.randint(1,8,8)\n",
    "    print(\"Softmax sorting   \", neat_vec(diff_sort(matrices, test, softmax=softmax)))\n",
    "    print(\"Posmax sorting    \", neat_vec(diff_sort(matrices, test, softmax=lambda a,b: softmax(a,b,normalize=1))))\n",
    "    print(\"Exact sorting     \", neat_vec(diff_sort(matrices, test, softmax=np.maximum)))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting complex numbers\n",
    "The softmax/smoothmax functions are analytic as well as differentiable, so \n",
    "we can also \"sort\" complex numbers. This approximately sorts them by the real part with smoothmax.\n",
    "Softmax forces the points towards the real line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x221cad52438>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test = np.random.randint(-200,200,8) + 1j * np.random.randint(-200, 200, 8)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def label_complex(zs, color, xyoffset, label):\n",
    "    ax.scatter(np.real(zs), np.imag(zs), c=color, label=label)    \n",
    "    for i, z in enumerate(zs):\n",
    "        ax.annotate(str(i), (np.real(z), np.imag(z)), color=color, textcoords=\"offset points\", xytext=xyoffset)\n",
    "\n",
    "        \n",
    "label_complex(test, 'C0', (0,5), label=\"Pre-sort\")\n",
    "\n",
    "smooth_sorted = diff_sort(matrices, test, softmax=smoothmax)\n",
    "label_complex(smooth_sorted, 'C1', (5, -5), \"Post-sort\")\n",
    "ax.axhline(0, c='k')\n",
    "ax.axvline(0, c='k')\n",
    "ax.set_xlabel(\"Real\")\n",
    "ax.set_ylabel(\"Imag\")\n",
    "ax.set_frame_on(False)\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relaxed sorting\n",
    "We can define a slighly modified function which interpolates between `softmax(a,b)` and `mean(a,b)`. The result is a sorting function that can be relaxed from sorting to averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact sorting             -110.00\t-67.00\t-65.00\t-50.00\t 14.00\t 95.00\t141.00\t151.00\n",
      "\n",
      "Softmax.   smooth[0.00]   -110.00\t-67.23\t-64.77\t-50.00\t 14.00\t 95.00\t141.00\t151.00\n",
      "Smoothmax. alpha=[1.00]   -110.00\t-66.49\t-65.51\t-50.00\t 14.00\t 95.00\t141.00\t151.00\n",
      "\n",
      "Softmax.   smooth[0.14]   -58.73\t-45.32\t-43.86\t-29.66\t 29.29\t 65.25\t 94.76\t 97.28\n",
      "Smoothmax. alpha=[0.86]   -110.00\t-66.37\t-65.63\t-50.00\t 14.00\t 95.00\t141.00\t151.00\n",
      "\n",
      "Softmax.   smooth[0.29]   -27.06\t-26.10\t-23.71\t-13.84\t 32.30\t 44.70\t 60.78\t 61.94\n",
      "Smoothmax. alpha=[0.71]   -110.00\t-66.25\t-65.75\t-50.00\t 14.00\t 95.00\t141.01\t150.99\n",
      "\n",
      "Softmax.   smooth[0.43]   -11.79\t-11.12\t -3.78\t -1.06\t 28.79\t 30.93\t 37.71\t 39.34\n",
      "Smoothmax. alpha=[0.57]   -110.00\t-66.15\t-65.85\t-50.00\t 14.00\t 95.00\t141.03\t150.97\n",
      "\n",
      "Softmax.   smooth[0.57]    -0.14\t  0.34\t  7.05\t  7.66\t 22.06\t 22.55\t 24.25\t 25.22\n",
      "Smoothmax. alpha=[0.43]   -110.00\t-66.05\t-65.91\t-50.03\t 14.00\t 95.00\t141.14\t150.86\n",
      "\n",
      "Softmax.   smooth[0.71]     8.17\t  8.48\t 11.79\t 12.10\t 16.30\t 16.61\t 17.60\t 17.93\n",
      "Smoothmax. alpha=[0.29]   -110.00\t-65.86\t-65.81\t-50.33\t 14.00\t 95.00\t141.54\t150.46\n",
      "\n",
      "Softmax.   smooth[0.86]    12.56\t 12.71\t 12.98\t 13.13\t 14.20\t 14.35\t 14.46\t 14.61\n",
      "Smoothmax. alpha=[0.14]   -109.84\t-64.59\t-64.47\t-53.10\t 13.99\t 95.08\t142.87\t149.05\n",
      "\n",
      "Softmax.   smooth[1.00]    13.62\t 13.62\t 13.62\t 13.62\t 13.62\t 13.62\t 13.62\t 13.62\n",
      "Smoothmax. alpha=[0.00]    13.62\t 13.62\t 13.62\t 13.62\t 13.62\t 13.62\t 13.62\t 13.62\n",
      "\n",
      "Mean 13.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Differentiable smoothed sorting \n",
    "test = np.random.randint(-200,200,8)\n",
    "\n",
    "print(\"Exact sorting            \", neat_vec(diff_sort(matrices, test, np.maximum)))\n",
    "print()\n",
    "for smooth in np.linspace(0, 1, 8):    \n",
    "    print(f\"Softmax.   smooth[{smooth:.2f}]  \", neat_vec(diff_sort(matrices, test, lambda a,b:softmax_smooth(a,b,smooth=smooth))))\n",
    "    # smoothmax's alpha is the inverse of diff_bisort_smooth\n",
    "    print(f\"Smoothmax. alpha=[{1-smooth:.2f}]  \", neat_vec(diff_sort(matrices, test, lambda a,b:smoothmax(a,b, alpha=1-smooth))))\n",
    "    print()\n",
    "    \n",
    "print(f\"Mean {np.mean(test):.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of the sorting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0016, 0.0009, 0.0024, 0.0259, 0.0531, 0.0531, 0.0282, 0.8346],\n",
       "       [0.026 , 0.0024, 0.0259, 0.0032, 0.8338, 0.0282, 0.0282, 0.0524],\n",
       "       [0.833 , 0.0523, 0.0024, 0.0258, 0.0275, 0.0281, 0.0274, 0.0033],\n",
       "       [0.0281, 0.0276, 0.0009, 0.0017, 0.0282, 0.8329, 0.0282, 0.0523],\n",
       "       [0.0524, 0.8344, 0.0532, 0.029 , 0.0017, 0.0261, 0.0017, 0.0017],\n",
       "       [0.004 , 0.0516, 0.8346, 0.0524, 0.0266, 0.0024, 0.0266, 0.0017],\n",
       "       [0.0267, 0.0032, 0.0275, 0.0282, 0.0274, 0.0266, 0.833 , 0.0274],\n",
       "       [0.0282, 0.0274, 0.0531, 0.8339, 0.0017, 0.0024, 0.0266, 0.0266]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from autograd import jacobian\n",
    "\n",
    "test = np.random.randint(-200,200,8)\n",
    "# show that we can take the derivative\n",
    "jac_sort = jacobian(diff_sort, argnum=1)\n",
    "jac_sort(matrices, test, softmax=lambda a,b:softmax_smooth(a,b,0.06)) # slight relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40.00\t -7.93\t -4.07\t 14.00\t 25.00\t 41.00\t141.00\t195.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.    , -0.    ,  0.    ,  0.    , -0.    ,  0.    ,  0.    ,\n",
       "         1.    ],\n",
       "       [-0.0908,  0.    , -0.    ,  0.    ,  1.091 , -0.0002, -0.    ,\n",
       "        -0.    ],\n",
       "       [ 1.0916, -0.    , -0.    , -0.    , -0.0909, -0.0008,  0.    ,\n",
       "         0.    ],\n",
       "       [-0.0009, -0.0182,  0.    , -0.    , -0.0001,  1.0192, -0.    ,\n",
       "        -0.    ],\n",
       "       [-0.    ,  1.023 , -0.0047, -0.    ,  0.    , -0.0182, -0.    ,\n",
       "         0.    ],\n",
       "       [ 0.    , -0.0047,  1.0047, -0.    , -0.    ,  0.    , -0.    ,\n",
       "        -0.    ],\n",
       "       [-0.    ,  0.    , -0.    , -0.    ,  0.    , -0.    ,  1.    ,\n",
       "        -0.    ],\n",
       "       [ 0.    , -0.    ,  0.    ,  1.    , -0.    ,  0.    , -0.    ,\n",
       "         0.    ]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same, but using smoothmax\n",
    "print(neat_vec(diff_sort(matrices, test, smoothmax)))\n",
    "jac_sort(matrices, test,  lambda a,b:smoothmax(a,b,alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woven form\n",
    "We can \"weave\" the four matrices into two matrices for fewer multiplies at the cost of having to split and join the matrices at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact sorting       -40.00\t -8.00\t -4.00\t 14.00\t 25.00\t 41.00\t141.00\t195.00\n",
      "Diff. (std.)        -40.00\t -7.93\t -4.07\t 14.00\t 25.00\t 41.00\t141.00\t195.00\n",
      "Diff. (woven)       -40.00\t -7.93\t -4.07\t 14.00\t 25.00\t 41.00\t141.00\t195.00\n"
     ]
    }
   ],
   "source": [
    "from differentiable_sorting import bitonic_woven_matrices, diff_sort_weave\n",
    "\n",
    "woven_matrices = bitonic_woven_matrices(8)\n",
    "\n",
    "print(\"Exact sorting      \", neat_vec(diff_sort(matrices, test, np.maximum)))\n",
    "print(f\"Diff. (std.)       \", neat_vec(diff_sort(matrices, test, smoothmax)))\n",
    "print(f\"Diff. (woven)      \", neat_vec(diff_sort_weave(woven_matrices, test, smoothmax)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable ranking / argsort\n",
    "We can use a differentiable similarity measure between the input and output of the vector, e.g. an RBF kernel. We can use this to generate a normalised similarity matrix and apply this to a vector `[1, 2, 3, ..., n]`. This gives a differentiable ranking function.\n",
    "\n",
    "As `sigma` gets larger, the result converges to giving all values the mean rank; as it goes to zero the result converges to the true rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting import order_matrix, diff_argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x               5.00\t -1.00\t  9.50\t 13.20\t 16.20\t 10.50\t 42.00\t 18.00\n",
      "diff_argsort    1.00\t  0.00\t  2.05\t  4.00\t  5.00\t  2.97\t  7.00\t  6.00\n",
      "exact argsort   1.00\t  0.00\t  2.00\t  5.00\t  3.00\t  4.00\t  7.00\t  6.00\n"
     ]
    }
   ],
   "source": [
    "x = [5.0, -1.0, 9.5, 13.2, 16.2, 10.5, 42.0, 18.0]\n",
    "\n",
    "\n",
    "print(\"x            \", neat_vec(x))\n",
    "# show argsort\n",
    "ranks = diff_argsort(matrices, x, sigma=0.5)\n",
    "print(\"diff_argsort \", neat_vec(ranks))\n",
    "print(\"exact argsort\",neat_vec(np.argsort(ranks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "# we now have differentiable argmax and argmin by indexing the rank vector\n",
    "print(np.argmin(x), int(ranks[0]+0.5))\n",
    "print(np.argmax(x), int(ranks[-1]+0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing ranks\n",
    "We can again relax argsort back to the mean rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed ranks\n",
      "sigma=    0.1  |   1.00\t  0.00\t  2.00\t  4.00\t  5.00\t  3.00\t  7.00\t  6.00\n",
      "sigma=    1.0  |   1.00\t  0.00\t  2.33\t  3.97\t  5.12\t  2.73\t  7.00\t  5.85\n",
      "sigma=   10.0  |   2.55\t  1.92\t  3.01\t  3.38\t  3.65\t  3.11\t  6.79\t  3.82\n",
      "sigma=  100.0  |   3.47\t  3.45\t  3.48\t  3.49\t  3.49\t  3.48\t  3.56\t  3.50\n",
      "sigma= 1000.0  |   3.50\t  3.50\t  3.50\t  3.50\t  3.50\t  3.50\t  3.50\t  3.50\n"
     ]
    }
   ],
   "source": [
    "print(\"Smoothed ranks\")\n",
    "test = x\n",
    "for sigma in [0.1, 1, 10, 100, 1000]:     \n",
    "    ranks = diff_argsort(matrices, test, sigma=sigma) \n",
    "    print(f\"sigma={sigma:7.1f}  |\", neat_vec(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0008 -0.     -0.0005 -0.     -0.     -0.0002 -0.     -0.    ]\n",
      " [-0.      0.     -0.     -0.     -0.     -0.     -0.     -0.    ]\n",
      " [-0.0026 -0.      0.2329 -0.0221 -0.0017 -0.2062 -0.     -0.0002]\n",
      " [-0.0007 -0.     -0.0307  0.1461 -0.0328 -0.0764 -0.     -0.0055]\n",
      " [-0.     -0.     -0.0016 -0.0319  0.2241 -0.0023 -0.     -0.1883]\n",
      " [-0.0029 -0.     -0.2093 -0.0656 -0.004   0.2827 -0.     -0.001 ]\n",
      " [-0.     -0.     -0.     -0.     -0.     -0.      0.     -0.    ]\n",
      " [-0.     -0.     -0.0009 -0.0128 -0.1914 -0.0023 -0.      0.2075]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "jac_rank = jacobian(diff_argsort, argnum=1)\n",
    "print(jac_rank(matrices, np.array(test), 1.0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_argsort   0.13\t  1.09\t  2.00\t  3.11\t  6.99\t  6.00\t  5.00\t  3.11\n",
      "argsort        0.00\t  1.00\t  2.00\t  3.00\t  7.00\t  6.00\t  5.00\t  4.00\n",
      "Jacobian of diff_argsort(x)\n",
      "[[ 2.1616 -1.0588 -0.5227 -0.2867 -0.0101 -0.0175 -0.0556 -0.2102]\n",
      " [-0.0658  0.5622 -0.1859 -0.1551 -0.0048 -0.0106 -0.0352 -0.1049]\n",
      " [-0.0121 -0.0131  0.0406 -0.0051 -0.0003 -0.0005 -0.0019 -0.0076]\n",
      " [-0.0123 -0.0252 -0.1084  0.5644 -0.0505 -0.0863 -0.1414 -0.1404]\n",
      " [-0.0006 -0.001  -0.0029 -0.0051  0.1036 -0.0579 -0.0285 -0.0077]\n",
      " [-0.0004 -0.0008 -0.0024 -0.004  -0.0015  0.0281 -0.0125 -0.0066]\n",
      " [-0.0002 -0.0003 -0.0008 -0.0015 -0.016  -0.0177  0.038  -0.0014]\n",
      " [-0.0123 -0.0252 -0.1084 -0.2095 -0.0505 -0.0863 -0.1414  0.6334]]\n"
     ]
    }
   ],
   "source": [
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "x = [1, 2, 3, 4, 8, 7, 6, 4]\n",
    "ranks = diff_argsort(matrices, x, sigma=0.25)\n",
    "print(\"diff_argsort\", neat_vec(ranks))\n",
    "print(\"argsort     \", neat_vec(np.argsort(ranks)))\n",
    "\n",
    "print(\"Jacobian of diff_argsort(x)\")\n",
    "print(jac_rank(matrices, np.array(x), 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x          41.00\t  2.00\t 30.00\t 40.00\t 50.00\t 60.00\t 70.00\t190.00\n",
      "d_rank/dx  -0.68\t -0.00\t -0.00\t  0.68\t -0.00\t -0.00\t -0.00\t  0.00\n"
     ]
    }
   ],
   "source": [
    "matrices = bitonic_matrices(8)\n",
    "from autograd import grad\n",
    "\n",
    "# which elements cause the biggest change in ranking if adjusted?\n",
    "# we can compute this directly:\n",
    "x = [41, 2, 30, 40, 50, 60, 70, 190]\n",
    "\n",
    "# approximate change in rank as first moment of ranks\n",
    "rank_change = lambda x: np.sum((diff_argsort(matrices, x, sigma=1.0)* np.arange(1,9)))\n",
    "grad_rank_change = grad(rank_change)\n",
    "print(\"x        \", neat_vec(x))\n",
    "print(\"d_rank/dx\", neat_vec(grad_rank_change(np.array(x))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting_torch import softmax, diff_argsort\n",
    "from differentiable_sorting import diff_sort\n",
    "matrices = bitonic_matrices(16)\n",
    "torch_matrices = [[torch.from_numpy(matrix).float().to(device) for matrix in matrix_set] for matrix_set in matrices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5973e-04, 3.3933e-02, 7.3613e-01, 5.7220e-06, 7.3584e-03, 1.8311e-02,\n",
      "         1.3363e-02, 2.3068e-03, 1.7365e-01, 5.7429e-03, 7.0045e-06, 2.8817e-05,\n",
      "         4.1973e-03, 3.6804e-03, 3.1447e-04, 5.0413e-04],\n",
      "        [4.8785e-04, 4.3964e-02, 1.9875e-01, 7.2420e-06, 1.1447e-02, 3.0540e-02,\n",
      "         2.0577e-02, 3.3834e-03, 6.5341e-01, 1.7145e-02, 2.0131e-05, 8.2905e-05,\n",
      "         9.5225e-03, 8.5303e-03, 8.1938e-04, 1.3184e-03],\n",
      "        [4.2167e-03, 2.7968e-01, 2.7102e-02, 5.4494e-05, 7.8724e-02, 2.0756e-01,\n",
      "         1.4958e-01, 2.2536e-02, 7.6766e-02, 4.5877e-02, 7.1609e-05, 3.0935e-04,\n",
      "         5.2884e-02, 4.5629e-02, 3.4320e-03, 5.5709e-03],\n",
      "        [5.1660e-03, 2.5247e-01, 2.4786e-02, 6.6891e-05, 7.6780e-02, 1.9704e-01,\n",
      "         1.5687e-01, 2.1487e-02, 5.0643e-02, 5.9974e-02, 7.6715e-05, 3.4016e-04,\n",
      "         7.7280e-02, 6.5229e-02, 4.5771e-03, 7.2146e-03],\n",
      "        [1.4384e-02, 9.4579e-02, 3.0093e-03, 1.8639e-04, 1.0764e-01, 1.2278e-01,\n",
      "         1.3674e-01, 5.6585e-02, 1.1777e-02, 1.3785e-01, 3.1020e-04, 1.3189e-03,\n",
      "         1.4208e-01, 1.3014e-01, 1.5501e-02, 2.5118e-02],\n",
      "        [1.4207e-02, 9.5981e-02, 3.1642e-03, 1.7407e-04, 1.0307e-01, 1.3132e-01,\n",
      "         1.4680e-01, 5.3570e-02, 1.1398e-02, 1.3862e-01, 3.4133e-04, 1.4380e-03,\n",
      "         1.3319e-01, 1.2384e-01, 1.6431e-02, 2.6465e-02],\n",
      "        [1.9670e-02, 7.5964e-02, 2.9877e-03, 2.6209e-04, 1.5066e-01, 9.9316e-02,\n",
      "         1.1669e-01, 9.0818e-02, 7.8987e-03, 1.4319e-01, 3.4490e-04, 1.5380e-03,\n",
      "         1.2555e-01, 1.2368e-01, 1.5249e-02, 2.6168e-02],\n",
      "        [1.9271e-02, 7.6437e-02, 3.0317e-03, 2.5127e-04, 1.4403e-01, 1.0184e-01,\n",
      "         1.1907e-01, 8.7000e-02, 7.9454e-03, 1.4730e-01, 3.5789e-04, 1.5925e-03,\n",
      "         1.2525e-01, 1.2390e-01, 1.5763e-02, 2.6961e-02],\n",
      "        [9.3685e-02, 1.3821e-02, 2.7234e-04, 2.0684e-03, 9.4396e-02, 2.3153e-02,\n",
      "         3.5742e-02, 1.5187e-01, 2.2822e-03, 1.0362e-01, 4.3838e-03, 1.4145e-02,\n",
      "         1.2054e-01, 1.3479e-01, 8.8949e-02, 1.1628e-01],\n",
      "        [9.3294e-02, 1.3716e-02, 2.8053e-04, 2.0780e-03, 9.5408e-02, 2.4164e-02,\n",
      "         3.7319e-02, 1.6534e-01, 2.0748e-03, 9.6028e-02, 4.6292e-03, 1.4848e-02,\n",
      "         1.0779e-01, 1.2088e-01, 9.6224e-02, 1.2592e-01],\n",
      "        [2.0925e-01, 9.2633e-03, 2.3339e-04, 5.0884e-03, 6.0492e-02, 2.0387e-02,\n",
      "         3.0750e-02, 1.4821e-01, 8.9527e-04, 4.6573e-02, 7.0969e-03, 2.6051e-02,\n",
      "         3.9016e-02, 4.5977e-02, 1.6370e-01, 1.8701e-01],\n",
      "        [2.7445e-01, 7.9823e-03, 1.9360e-04, 6.2927e-03, 5.3573e-02, 1.7542e-02,\n",
      "         2.6823e-02, 1.3388e-01, 9.1309e-04, 4.3353e-02, 7.3285e-03, 2.8382e-02,\n",
      "         4.1861e-02, 4.8357e-02, 1.5125e-01, 1.5782e-01],\n",
      "        [1.7331e-01, 1.2653e-03, 3.3603e-05, 2.6744e-02, 9.5518e-03, 3.7706e-03,\n",
      "         6.0474e-03, 4.0170e-02, 2.1345e-04, 8.5532e-03, 7.3450e-02, 1.9766e-01,\n",
      "         1.1754e-02, 1.4452e-02, 2.5436e-01, 1.7867e-01],\n",
      "        [5.8270e-02, 6.7665e-04, 1.9530e-05, 7.3802e-02, 5.5178e-03, 1.8113e-03,\n",
      "         2.9148e-03, 1.8950e-02, 9.8627e-05, 4.5468e-03, 1.4987e-01, 4.3007e-01,\n",
      "         7.1183e-03, 8.5647e-03, 1.4334e-01, 9.4420e-02],\n",
      "        [1.0241e-02, 1.3782e-04, 3.3124e-06, 4.7362e-01, 6.9566e-04, 2.3866e-04,\n",
      "         3.6516e-04, 1.9891e-03, 1.5358e-05, 7.9039e-04, 3.4993e-01, 1.3507e-01,\n",
      "         9.5657e-04, 1.1424e-03, 1.4739e-02, 1.0064e-02],\n",
      "        [9.6351e-03, 1.2702e-04, 3.0925e-06, 4.0930e-01, 6.5626e-04, 2.2528e-04,\n",
      "         3.4585e-04, 1.9014e-03, 1.6262e-05, 8.3955e-04, 4.0178e-01, 1.4713e-01,\n",
      "         1.0046e-03, 1.1988e-03, 1.5341e-02, 1.0504e-02]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input = np.random.normal(0, 5, 16)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device),\n",
    "                          requires_grad=True)\n",
    "\n",
    "result = diff_sort(torch_matrices, var_test_input, softmax=softmax)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the\n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(\n",
    "        torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 16 x 16 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.0000,  3.4317,  0.0000, 14.0000,  6.0228,  3.5761,  4.7984,  9.0000,\n",
      "         1.0000,  5.9649, 14.0000, 13.0000,  5.9053,  7.6375, 11.0000, 10.0000],\n",
      "       device='cuda:0', grad_fn=<MvBackward>)\n"
     ]
    }
   ],
   "source": [
    "result = diff_argsort(torch_matrices, var_test_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
