{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations executed in parallel. A sorting network implements  a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$ (sorting networks for non-power-of-2 sizes are possible but not trickier).\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network for $n=2^k$ elements has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a $k$ element vector into sorted order.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification!) as the continuous approximation to the `max(a,b)` function:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b) \\approx \\max(a,b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin(a,b)` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b}) \\approx \\min(a,b).$$ More numerically stably we can write: \n",
    "\n",
    "$$\\text{softmin}(a,b) = a + b - \\text{softmax}(a,b).$$\n",
    "\n",
    "These functions obviously aren't equal to max and min, but are relatively close, and differentiable. Note that we now have a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{high} = \\text{softmax}(a,b), \\text{low} = \\text{softmin}(a,b), \\text{where } \\text{low}\\leq \\text{high}$$\n",
    "\n",
    "Alternatively, we can use: \n",
    "$$\\text{smoothmax}(a,b) = \\frac{a (e^{\\alpha a}) + b (e^{\\alpha b})}{e^{\\alpha a}+e^{\\alpha b}}  \\approx \\max(a,b).$$  This has an adjustable smoothness parameter $\\alpha$, with exact maximum as $\\alpha \\rightarrow \\infty$ and pure averaging as $\\alpha \\rightarrow 0$.\n",
    "\n",
    "## Differentiable sorting\n",
    "\n",
    "For each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides which can be done simultaneously. We can any write function that selects the relevant elements of the vector as a multiply with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ and $R \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{k}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ and $R' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i[\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i})] + R'_i[\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})]$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is clearly differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network. \n",
    "\n",
    "This process is excessively computation heavy, but easy to compute. We could also simplify this into two matrix multiplies, at the cost of a vector split and join in the middle (see the `woven` form later in this text). \n",
    "\n",
    "## Example\n",
    "\n",
    "To sort four elements, we have a network like:\n",
    "\n",
    "    0  1  2  3  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕<<┙  \n",
    "    ┕>>>>>┙  │  \n",
    "    │  │  │  │  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕>>┙  \n",
    "    \n",
    "This is equivalent to: \n",
    "\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[3], x[2] = cswap(x[2], x[3])\n",
    "    x[0], x[2] = cswap(x[0], x[2])\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[2], x[3] = cswap(x[2], x[3])\n",
    "    \n",
    "where `cswap(a,b) = (min(a,b), max(a,b))`\n",
    "\n",
    "Replacing the indexing with matrix multiplies and `cswap` with a `softcswap = (softmin(a,b), softmax(a,b))` we then have the differentiable form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0>1\t 2<3\t 4>5\t 6<7\t 8>9\t10<11\t12>13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>2\t 1>3\t 4<6\t 5<7\t 8>10\t 9>11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4<5\t 6<7\t 8>9\t10>11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8<12\t 9<13\t10<14\t11<15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8<10\t 9<11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8<9\t10<11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>8\t 1>9\t 2>10\t 3>11\t 4>12\t 5>13\t 6>14\t 7>15\t\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8>12\t 9>13\t10>14\t11>15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8>10\t 9>11\t12>14\t13>15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8>9\t10>11\t12>13\t14>15\t\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from bitonic_tests import bitonic_network, pretty_bitonic_network\n",
    "\n",
    "def neat_vec(n):\n",
    "    # print a vector neatly    \n",
    "    return \"\\t\".join([f\"{x:.2f}\" for x in n])\n",
    "\n",
    "# this should match the diagram at the top of the notebook\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15\n",
      " ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  ╰──╮  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  ╰──╮  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╭──╯  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  ╰──╮  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╭──╯  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  │  ╰──╮ \n",
      " ╭─────╯  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╰─────╮  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  ╰─────╮  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╭─────╯  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  ╭─────╯  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╰─────╮  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  ╰─────╮ \n",
      " ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╰──╮  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  ╰──╮  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╭──╯  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  ╭──╯  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╰──╮  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  │  ╰──╮ \n",
      " ╭───────────╯  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  ╭───────────╯  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  ╭───────────╯  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  ╭───────────╯  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╰───────────╮  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  ╰───────────╮  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  ╰───────────╮  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  ╰───────────╮ \n",
      " ╭─────╯  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╭─────╯  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  ╭─────╯  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╰─────╮  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  ╰─────╮  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╰─────╮  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  ╰─────╮ \n",
      " ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  ╭──╯  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╰──╮  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  ╰──╮  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╰──╮  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  │  ╰──╮ \n",
      " ╭───────────────────────╯  │  │  │  │  │  │  │ \n",
      " │  ╭───────────────────────╯  │  │  │  │  │  │ \n",
      " │  │  ╭───────────────────────╯  │  │  │  │  │ \n",
      " │  │  │  ╭───────────────────────╯  │  │  │  │ \n",
      " │  │  │  │  ╭───────────────────────╯  │  │  │ \n",
      " │  │  │  │  │  ╭───────────────────────╯  │  │ \n",
      " │  │  │  │  │  │  ╭───────────────────────╯  │ \n",
      " │  │  │  │  │  │  │  ╭───────────────────────╯ \n",
      " ╭───────────╯  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  ╭───────────╯  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  ╭───────────╯  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  ╭───────────╯  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╭───────────╯  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  ╭───────────╯  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  ╭───────────╯  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  ╭───────────╯ \n",
      " ╭─────╯  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  ╭─────╯  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╭─────╯  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  ╭─────╯  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╭─────╯  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  ╭─────╯  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╭─────╯  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  ╭─────╯ \n",
      " ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  ╭──╯  │  │  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  ╭──╯  │  │  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  ╭──╯  │  │  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  ╭──╯  │  │  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  ╭──╯  │  │  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  ╭──╯  │  │ \n",
      " │  │  │  │  │  │  │  │  │  │  │  │  │  │  ╭──╯ \n"
     ]
    }
   ],
   "source": [
    "pretty_bitonic_network(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.  17.  35.  65. 127. 134. 148. 155.]\n",
      "[ 44.  48.  50.  80.  80. 120. 125. 135.]\n",
      "[  5.   5.  48.  49.  82. 130. 164. 171.]\n",
      "[ 51.  83.  94. 104. 127. 136. 136. 173.]\n",
      "[  0.  13.  71.  84. 103. 132. 166. 181.]\n",
      "[ 12.  12.  23.  66. 102. 111. 124. 151.]\n",
      "[ 92.  99. 115. 120. 142. 156. 164. 174.]\n",
      "[ 24.  52.  87. 122. 125. 130. 170. 179.]\n",
      "[  5.  11.  36.  37.  63.  92. 146. 148.]\n",
      "[ 27.  59.  59.  97. 131. 153. 162. 184.]\n"
     ]
    }
   ],
   "source": [
    "# Test sorting\n",
    "import autograd.numpy as np # we can use plain numpy as well (but can't take grad!)\n",
    "\n",
    "\n",
    "from differentiable_sorting import bitonic_matrices, diff_bisort, diff_argsort, softmax, softmin, softcswap, bisort, smoothmax_bisort\n",
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "# test bitonic sorting with exact maximum\n",
    "for i in range(10):\n",
    "    # these should all be in sorted order\n",
    "    test = np.random.randint(0, 200, 8)\n",
    "    print(bisort(matrices, test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sorting for 2 elements\n",
      "Testing sorting for 4 elements\n",
      "Testing sorting for 8 elements\n",
      "Testing sorting for 16 elements\n",
      "Testing sorting for 32 elements\n",
      "Testing sorting for 64 elements\n",
      "Testing sorting for 128 elements\n",
      "Testing sorting for 256 elements\n",
      "Testing sorting for 512 elements\n",
      "Testing sorting for 1024 elements\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    k = 2**i\n",
    "    matrices = bitonic_matrices(k)\n",
    "    print(f\"Testing sorting for {k} elements\")\n",
    "    for j in range(100):\n",
    "        test = np.random.randint(0, 200, k)\n",
    "\n",
    "        assert (np.allclose(bisort(matrices, test), np.sort(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax sorting    -155.00\t-115.00\t-74.00\t-56.00\t-49.00\t-14.00\t5.00\t192.00\n",
      "Smoothmax sorting  -155.00\t-115.00\t-74.00\t-55.99\t-49.01\t-14.00\t5.00\t192.00\n",
      "Exact sorting      -155.00\t-115.00\t-74.00\t-56.00\t-49.00\t-14.00\t5.00\t192.00\n",
      "\n",
      "Softmax sorting    -197.00\t-174.00\t-129.00\t-119.00\t-106.00\t-65.00\t81.00\t148.00\n",
      "Smoothmax sorting  -197.00\t-174.00\t-129.00\t-119.00\t-106.00\t-65.00\t81.00\t148.00\n",
      "Exact sorting      -197.00\t-174.00\t-129.00\t-119.00\t-106.00\t-65.00\t81.00\t148.00\n",
      "\n",
      "Softmax sorting    -154.00\t-115.00\t-75.00\t-2.00\t33.00\t78.00\t146.00\t172.00\n",
      "Smoothmax sorting  -154.00\t-115.00\t-75.00\t-2.00\t33.00\t78.00\t146.00\t172.00\n",
      "Exact sorting      -154.00\t-115.00\t-75.00\t-2.00\t33.00\t78.00\t146.00\t172.00\n",
      "\n",
      "Softmax sorting    -135.00\t-75.00\t-46.00\t11.00\t18.00\t54.00\t104.00\t111.00\n",
      "Smoothmax sorting  -135.00\t-75.00\t-46.00\t11.01\t17.99\t54.00\t104.01\t110.99\n",
      "Exact sorting      -135.00\t-75.00\t-46.00\t11.00\t18.00\t54.00\t104.00\t111.00\n",
      "\n",
      "Softmax sorting    -183.00\t-123.00\t-69.00\t-0.00\t10.00\t85.00\t117.00\t180.00\n",
      "Smoothmax sorting  -183.00\t-123.00\t-69.00\t0.00\t10.00\t85.00\t117.00\t180.00\n",
      "Exact sorting      -183.00\t-123.00\t-69.00\t0.00\t10.00\t85.00\t117.00\t180.00\n",
      "\n",
      "Softmax sorting    -187.00\t-53.00\t-32.00\t39.00\t99.00\t107.00\t142.00\t188.00\n",
      "Smoothmax sorting  -187.00\t-53.00\t-32.00\t39.00\t99.01\t106.99\t142.00\t188.00\n",
      "Exact sorting      -187.00\t-53.00\t-32.00\t39.00\t99.00\t107.00\t142.00\t188.00\n",
      "\n",
      "Softmax sorting    -170.00\t-158.05\t-155.08\t-152.86\t-27.00\t45.00\t105.00\t193.00\n",
      "Smoothmax sorting  -170.00\t-157.82\t-154.92\t-153.26\t-27.00\t45.00\t105.00\t193.00\n",
      "Exact sorting      -170.00\t-158.00\t-155.00\t-153.00\t-27.00\t45.00\t105.00\t193.00\n",
      "\n",
      "Softmax sorting    -103.00\t-67.00\t-60.00\t23.00\t30.69\t32.31\t68.95\t72.05\n",
      "Smoothmax sorting  -103.00\t-66.99\t-60.01\t23.00\t31.27\t31.73\t69.14\t71.86\n",
      "Exact sorting      -103.00\t-67.00\t-60.00\t23.00\t31.00\t32.00\t69.00\t72.00\n",
      "\n",
      "Softmax sorting    -200.00\t-61.00\t6.00\t19.00\t37.00\t130.00\t154.00\t189.00\n",
      "Smoothmax sorting  -200.00\t-61.00\t6.00\t19.00\t37.00\t130.00\t154.00\t189.00\n",
      "Exact sorting      -200.00\t-61.00\t6.00\t19.00\t37.00\t130.00\t154.00\t189.00\n",
      "\n",
      "Softmax sorting    -72.00\t-58.00\t-51.00\t31.00\t133.00\t148.00\t166.00\t199.00\n",
      "Smoothmax sorting  -72.00\t-57.99\t-51.01\t31.00\t133.00\t148.00\t166.00\t199.00\n",
      "Exact sorting      -72.00\t-58.00\t-51.00\t31.00\t133.00\t148.00\t166.00\t199.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differentiable sorting \n",
    "np.set_printoptions(precision=2)\n",
    "matrices = bitonic_matrices(8) \n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    test = np.random.randint(-200,200,8)\n",
    "    print(\"Softmax sorting   \", neat_vec(diff_bisort(matrices, test)))\n",
    "    print(\"Smoothmax sorting \", neat_vec(smoothmax_bisort(matrices, test)))\n",
    "    print(\"Exact sorting     \", neat_vec(bisort(matrices, test)))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relaxed sorting\n",
    "We can define a slighly modified function which interpolates between `softmax(a,b)` and `mean(a,b)`. The result is a sorting function that can be relaxed from sorting to averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean -93.50\n",
      "\n",
      "Exact sorting             -163.00\t-152.00\t-143.00\t-141.00\t-99.00\t-78.00\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.00]   -163.00\t-152.00\t-143.13\t-140.87\t-99.00\t-78.00\t-22.00\t50.00\n",
      "Smoothmax. alpha=[1.00]   -163.00\t-152.00\t-142.76\t-141.24\t-99.00\t-78.00\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.14]   -141.18\t-137.42\t-135.56\t-118.78\t-90.26\t-84.05\t-39.61\t-1.15\n",
      "Smoothmax. alpha=[0.86]   -163.00\t-152.00\t-142.70\t-141.31\t-99.00\t-78.00\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.29]   -127.49\t-126.38\t-123.81\t-111.04\t-89.36\t-78.39\t-55.60\t-35.93\n",
      "Smoothmax. alpha=[0.71]   -162.99\t-151.99\t-142.63\t-141.39\t-99.00\t-78.00\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.43]   -118.72\t-118.03\t-115.72\t-112.68\t-80.87\t-76.41\t-66.82\t-58.75\n",
      "Smoothmax. alpha=[0.57]   -162.96\t-151.97\t-142.57\t-141.51\t-99.00\t-78.00\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.57]   -111.39\t-110.89\t-109.95\t-109.40\t-79.41\t-78.15\t-75.48\t-73.34\n",
      "Smoothmax. alpha=[0.43]   -162.81\t-151.90\t-142.59\t-141.71\t-99.00\t-78.00\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.71]   -104.93\t-104.62\t-104.44\t-104.13\t-82.88\t-82.52\t-82.42\t-82.04\n",
      "Smoothmax. alpha=[0.29]   -162.00\t-151.73\t-143.01\t-142.26\t-98.95\t-78.05\t-22.00\t50.00\n",
      "\n",
      "Softmax.   smooth[0.86]   -98.82\t-98.67\t-98.66\t-98.51\t-88.48\t-88.33\t-88.33\t-88.18\n",
      "Smoothmax. alpha=[0.14]   -156.47\t-151.96\t-146.00\t-144.39\t-98.18\t-78.98\t-22.02\t50.00\n",
      "\n",
      "Softmax.   smooth[1.00]   -93.50\t-93.50\t-93.50\t-93.50\t-93.50\t-93.50\t-93.50\t-93.50\n",
      "Smoothmax. alpha=[0.00]   -93.50\t-93.50\t-93.50\t-93.50\t-93.50\t-93.50\t-93.50\t-93.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from differentiable_sorting import diff_bisort_smooth\n",
    "# Differentiable smoothed sorting \n",
    "test = np.random.randint(-200,200,8)\n",
    "print(f\"Mean {np.mean(test):.2f}\")\n",
    "print()\n",
    "print(\"Exact sorting            \", neat_vec(bisort(matrices, test)))\n",
    "print()\n",
    "for smooth in np.linspace(0, 1, 8):    \n",
    "    print(f\"Softmax.   smooth[{smooth:.2f}]  \", neat_vec(diff_bisort_smooth(matrices, test, smooth)))\n",
    "    # smoothmax's alpha is the inverse of diff_bisort_smooth\n",
    "    print(f\"Smoothmax. alpha=[{1-smooth:.2f}]  \", neat_vec(smoothmax_bisort(matrices, test, alpha=1-smooth)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.02, 0.  , 0.  , 0.05, 0.86, 0.05, 0.02],\n",
       "       [0.02, 0.  , 0.  , 0.02, 0.03, 0.05, 0.86, 0.02],\n",
       "       [0.02, 0.  , 0.02, 0.  , 0.86, 0.04, 0.03, 0.02],\n",
       "       [0.86, 0.02, 0.02, 0.02, 0.02, 0.  , 0.02, 0.02],\n",
       "       [0.02, 0.04, 0.86, 0.02, 0.02, 0.  , 0.  , 0.02],\n",
       "       [0.02, 0.  , 0.02, 0.02, 0.02, 0.02, 0.02, 0.86],\n",
       "       [0.02, 0.04, 0.02, 0.86, 0.  , 0.  , 0.02, 0.02],\n",
       "       [0.02, 0.86, 0.05, 0.05, 0.  , 0.02, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autograd import jacobian\n",
    "# show that we can take the derivative\n",
    "jac_sort = jacobian(diff_bisort_smooth, argnum=1)\n",
    "jac_sort(matrices, test, 0.05) # slight relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-159.94 -151.72 -144.13 -143.2   -98.7   -78.31  -22.     50.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.18e-03, -2.98e-16,  5.63e-05, -1.62e-10, -1.43e-01,  1.38e+00,\n",
       "        -2.43e-01,  1.59e-06],\n",
       "       [-1.06e-01,  9.30e-16,  1.69e-04, -5.51e-10, -7.86e-02, -2.40e-01,\n",
       "         1.42e+00, -3.18e-06],\n",
       "       [ 2.30e-01,  3.55e-15, -1.13e-03,  3.10e-09,  9.62e-01, -1.14e-01,\n",
       "        -7.74e-02, -2.55e-05],\n",
       "       [ 8.68e-01,  5.00e-15, -1.64e-03,  3.94e-09,  2.60e-01, -2.30e-02,\n",
       "        -1.04e-01, -3.79e-05],\n",
       "       [-1.74e-03, -5.51e-11,  1.05e+00,  3.36e-06, -1.04e-03,  5.19e-05,\n",
       "         6.51e-05, -4.63e-02],\n",
       "       [ 3.60e-05,  1.09e-09, -4.64e-02, -1.46e-04,  2.24e-05, -1.90e-06,\n",
       "        -6.79e-06,  1.05e+00],\n",
       "       [ 4.42e-09, -1.49e-05, -2.95e-06,  1.00e+00,  3.15e-09, -3.95e-11,\n",
       "         3.63e-10, -1.39e-04],\n",
       "       [-2.07e-14,  1.00e+00,  3.75e-11, -1.49e-05, -2.02e-14,  1.12e-16,\n",
       "        -2.91e-15,  1.04e-09]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show that we can take the derivative, applying some smoothing to get reasonable values\n",
    "jac_sort = jacobian(smoothmax_bisort, argnum=1)\n",
    "print(smoothmax_bisort(matrices, test, 0.2))\n",
    "jac_sort(matrices, test, 0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woven form\n",
    "We can \"weave\" the four matrices into two matrices for fewer multiplies at the cost of having to split and join the matrices at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact sorting        -176.00\t-157.00\t-123.00\t-123.00\t-118.00\t-35.00\t-15.00\t42.00\n",
      "Diff. (std.)        -176.00\t-157.00\t-123.92\t-122.09\t-117.99\t-35.00\t-15.00\t42.00\n",
      "Diff. (woven)       -176.00\t-157.00\t-123.92\t-122.09\t-117.99\t-35.00\t-15.00\t42.00\n"
     ]
    }
   ],
   "source": [
    "from differentiable_sorting import bitonic_woven_matrices\n",
    "\n",
    "def diff_bisort_weave(matrices, x):\n",
    "    \"\"\"\n",
    "    Given a set of bitonic sort matrices generated by bitonic_woven_matrices(n), sort \n",
    "    a sequence x of length n.\n",
    "    \"\"\"\n",
    "    split = len(x) // 2\n",
    "    for weave, unweave in matrices:\n",
    "        woven = weave @ x\n",
    "        x = unweave @ np.concatenate(softcswap(woven[:split], woven[split:]))\n",
    "    return x\n",
    "\n",
    "\n",
    "woven_matrices = bitonic_woven_matrices(8)\n",
    "\n",
    "print(\"Exact sorting       \", neat_vec(bisort(matrices, test)))\n",
    "print(f\"Diff. (std.)       \", neat_vec(diff_bisort(matrices, test)))\n",
    "print(f\"Diff. (woven)      \", neat_vec(diff_bisort_weave(woven_matrices, test)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable ranking / argsort\n",
    "We can use a differentiable similarity measure between the input and output of the vector, e.g. an RBF kernel. We can use this to generate a normalised similarity matrix and apply this to a vector `[1, 2, 3, ..., n]`. This gives a differentiable ranking function.\n",
    "\n",
    "As `sigma` gets larger, the result converges to giving all values the mean rank; as it goes to zero the result converges to the true rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting import order_matrix, diff_argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, -1.0, 9.5, 13.2, 16.2, 10.5, 42.0, 18.0]\n",
      "1.00\t0.00\t2.00\t4.00\t5.00\t3.00\t7.00\t6.00\n"
     ]
    }
   ],
   "source": [
    "x = [5.0, -1.0, 9.5, 13.2, 16.2, 10.5, 42.0, 18.0]\n",
    "np.set_printoptions(suppress=True)\n",
    "print(x)\n",
    "# show argsort\n",
    "ranks = diff_argsort(matrices, x, sigma=0.1)\n",
    "print(neat_vec(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "# we now have differentiable argmax and argmin by indexing the rank vector\n",
    "print(np.argmin(x), int(ranks[0]))\n",
    "print(np.argmax(x), int(ranks[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed ranks\n",
      "sigma=    0.1  | 1.00\t0.00\t2.00\t4.00\t5.00\t3.00\t7.00\t6.00\n",
      "sigma=    1.0  | 1.00\t0.00\t2.33\t3.97\t5.12\t2.73\t7.00\t5.85\n",
      "sigma=   10.0  | 2.55\t1.92\t3.01\t3.38\t3.65\t3.11\t6.79\t3.82\n",
      "sigma=  100.0  | 3.47\t3.45\t3.48\t3.49\t3.49\t3.48\t3.56\t3.50\n",
      "sigma= 1000.0  | 3.50\t3.50\t3.50\t3.50\t3.50\t3.50\t3.50\t3.50\n"
     ]
    }
   ],
   "source": [
    "print(\"Smoothed ranks\")\n",
    "test = x\n",
    "for sigma in [0.1, 1, 10, 100, 1000]:     \n",
    "    ranks = diff_argsort(matrices, test, sigma=sigma) \n",
    "    print(f\"sigma={sigma:7.1f}  |\", neat_vec(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.001 -0.    -0.001 -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.     0.    -0.    -0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.003 -0.     0.233 -0.022 -0.002 -0.206 -0.    -0.   ]\n",
      " [-0.001 -0.    -0.031  0.146 -0.033 -0.076 -0.    -0.005]\n",
      " [-0.    -0.    -0.002 -0.032  0.224 -0.002 -0.    -0.188]\n",
      " [-0.003 -0.    -0.209 -0.066 -0.004  0.283 -0.    -0.001]\n",
      " [-0.    -0.    -0.    -0.    -0.    -0.     0.    -0.   ]\n",
      " [-0.    -0.    -0.001 -0.013 -0.191 -0.002 -0.     0.207]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "jac_rank = jacobian(diff_argsort, argnum=1)\n",
    "print(jac_rank(matrices, np.array(test), 1.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting_torch import diff_bisort, bitonic_matrices, diff_argsort\n",
    "matrices = bitonic_matrices(16)\n",
    "torch_matrices = [[torch.from_numpy(matrix).float().to(device) for matrix in matrix_set] for matrix_set in matrices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7075e-03, 1.6772e-05, 9.5515e-01, 3.1831e-05, 2.1090e-02, 1.5687e-02,\n",
      "         6.0490e-05, 9.9590e-07, 2.2132e-05, 2.8194e-05, 2.2722e-04, 3.4077e-04,\n",
      "         2.9672e-05, 1.6629e-04, 2.0245e-04, 2.4289e-04],\n",
      "        [1.0620e-01, 1.9133e-04, 3.4910e-02, 3.4753e-04, 4.8142e-01, 3.2459e-01,\n",
      "         9.9611e-04, 1.7100e-05, 8.8798e-04, 1.1313e-03, 9.4041e-03, 1.4216e-02,\n",
      "         1.1910e-03, 6.6996e-03, 8.1069e-03, 9.6946e-03],\n",
      "        [3.0832e-01, 7.1638e-04, 5.1102e-03, 1.3963e-03, 2.1782e-01, 3.0067e-01,\n",
      "         2.5627e-03, 4.4879e-05, 2.4988e-03, 3.1937e-03, 2.9483e-02, 4.7383e-02,\n",
      "         3.5332e-03, 1.9902e-02, 2.5691e-02, 3.1670e-02],\n",
      "        [2.8261e-01, 5.1883e-04, 3.5821e-03, 1.0642e-03, 1.6873e-01, 2.1003e-01,\n",
      "         2.0646e-03, 4.5296e-05, 4.9956e-03, 6.3725e-03, 6.0788e-02, 9.8363e-02,\n",
      "         7.0049e-03, 3.9861e-02, 5.1130e-02, 6.2835e-02],\n",
      "        [1.4036e-01, 4.5704e-03, 5.2332e-04, 7.1419e-03, 4.9832e-02, 6.7240e-02,\n",
      "         1.1709e-02, 3.2598e-04, 1.1847e-02, 1.5670e-02, 1.2850e-01, 1.7676e-01,\n",
      "         1.5877e-02, 1.0462e-01, 1.2180e-01, 1.4323e-01],\n",
      "        [6.8006e-02, 8.6579e-03, 2.7879e-04, 1.2596e-02, 2.7530e-02, 3.7736e-02,\n",
      "         2.3496e-02, 6.3195e-04, 1.1776e-02, 1.5592e-02, 1.4319e-01, 2.0366e-01,\n",
      "         1.7095e-02, 1.1686e-01, 1.4203e-01, 1.7086e-01],\n",
      "        [4.3686e-02, 1.4238e-02, 2.4107e-04, 2.4224e-02, 1.7294e-02, 2.1881e-02,\n",
      "         4.0981e-02, 9.2838e-04, 2.0402e-02, 2.7672e-02, 1.5064e-01, 1.3651e-01,\n",
      "         2.3073e-02, 1.5953e-01, 1.5514e-01, 1.6356e-01],\n",
      "        [1.8841e-02, 3.1060e-02, 1.2739e-04, 5.0848e-02, 8.1629e-03, 1.0679e-02,\n",
      "         9.9987e-02, 2.1356e-03, 1.8284e-02, 2.4790e-02, 1.3849e-01, 1.2979e-01,\n",
      "         2.0959e-02, 1.4659e-01, 1.4477e-01, 1.5450e-01],\n",
      "        [1.0371e-02, 4.4471e-02, 3.5313e-05, 6.4574e-02, 3.4811e-03, 4.7689e-03,\n",
      "         8.9456e-02, 3.9232e-03, 1.0591e-01, 1.2105e-01, 9.9584e-02, 5.5300e-02,\n",
      "         1.1638e-01, 1.0916e-01, 1.0127e-01, 7.0262e-02],\n",
      "        [6.0705e-03, 6.8355e-02, 1.9843e-05, 8.9537e-02, 2.0099e-03, 2.8219e-03,\n",
      "         1.4946e-01, 5.5270e-03, 9.7199e-02, 1.1273e-01, 8.1939e-02, 4.7734e-02,\n",
      "         9.9199e-02, 9.0939e-02, 8.3348e-02, 6.3110e-02],\n",
      "        [2.9441e-03, 8.8979e-02, 9.4913e-06, 1.3836e-01, 9.9704e-04, 1.4420e-03,\n",
      "         1.3920e-01, 5.7749e-03, 1.1886e-01, 1.2503e-01, 5.0781e-02, 2.8997e-02,\n",
      "         1.4260e-01, 6.1979e-02, 5.3150e-02, 4.0895e-02],\n",
      "        [2.6971e-03, 8.8836e-02, 8.5894e-06, 1.3126e-01, 9.1367e-04, 1.3331e-03,\n",
      "         1.4206e-01, 5.9859e-03, 1.1731e-01, 1.2607e-01, 5.3476e-02, 3.0712e-02,\n",
      "         1.3642e-01, 6.4243e-02, 5.5509e-02, 4.3166e-02],\n",
      "        [1.1219e-03, 1.9130e-01, 3.0259e-06, 1.7280e-01, 2.8566e-04, 4.4604e-04,\n",
      "         1.0898e-01, 2.9652e-02, 1.4371e-01, 1.2505e-01, 1.5962e-02, 9.0329e-03,\n",
      "         1.4324e-01, 2.5627e-02, 1.8205e-02, 1.4569e-02],\n",
      "        [1.1126e-03, 1.9533e-01, 2.9827e-06, 1.7081e-01, 2.7784e-04, 4.3405e-04,\n",
      "         1.0699e-01, 2.9468e-02, 1.4467e-01, 1.2575e-01, 1.6114e-02, 9.1172e-03,\n",
      "         1.4152e-01, 2.5578e-02, 1.8243e-02, 1.4581e-02],\n",
      "        [7.0988e-04, 2.0443e-01, 1.7324e-06, 9.8704e-02, 9.7413e-05, 1.5653e-04,\n",
      "         5.4837e-02, 1.1837e-01, 1.7594e-01, 1.4802e-01, 1.8529e-02, 1.0448e-02,\n",
      "         1.1264e-01, 2.4252e-02, 1.8399e-02, 1.4463e-02],\n",
      "        [2.4536e-04, 5.8327e-02, 6.2260e-07, 3.6306e-02, 5.4205e-05, 8.2959e-05,\n",
      "         2.7149e-02, 7.9716e-01, 2.5687e-02, 2.1840e-02, 2.8968e-03, 1.6397e-03,\n",
      "         1.9241e-02, 4.0047e-03, 3.0065e-03, 2.3543e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input = np.random.normal(0, 5, 16)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device),\n",
    "                          requires_grad=True)\n",
    "\n",
    "result = diff_bisort(torch_matrices, var_test_input)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the\n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(\n",
    "        torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 16 x 16 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000e+00, 4.0000e+00, 4.9946e+00, 2.6924e-36, 3.1385e-26, 1.1046e+01,\n",
      "        1.5000e+01, 1.3000e+01, 1.3078e+01, 6.0000e+00, 6.0000e+00, 8.0001e+00,\n",
      "        7.7404e+00, 1.0000e+00, 1.0000e+00, 7.0000e+00], device='cuda:0',\n",
      "       grad_fn=<MvBackward>)\n"
     ]
    }
   ],
   "source": [
    "result = diff_argsort(torch_matrices, var_test_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
