{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations. This is a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$.\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a vector.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax` function (not the traditional \"softmax\" used for classification!) as:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b})$$\n",
    "\n",
    "These aren't *exactly* equal to max and min, but are relatively close, and differentiable.\n",
    "\n",
    "Note that we can now do a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{high} = \\text{softmax}(a,b), \\text{low} = \\text{softmin}(a,b), \\text{where } \\text{low}\\leq {high}$$\n",
    "\n",
    "## Differentiable sorting.\n",
    "\n",
    "For each layer in the sorting network, we can split all of the comparisons into the left-hand and right-hand sides. We can any function that selects the relevant elements of the vector as a multipy with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{k \\times \\frac{k}{2}}$ and $R \\in \\mathbb{R}^{k \\times \\frac{k}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{k}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{k}{2} \\times k}$ and $R' \\in \\mathbb{R}^{\\frac{k}{2} \\times k}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i}) + R'_i\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is clearly differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0>1\t 2<3\t 4>5\t 6<7\t 8>9\t10<11\t12>13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>2\t 1>3\t 4<6\t 5<7\t 8>10\t 9>11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4<5\t 6<7\t 8>9\t10>11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8<12\t 9<13\t10<14\t11<15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8<10\t 9<11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8<9\t10<11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>8\t 1>9\t 2>10\t 3>11\t 4>12\t 5>13\t 6>14\t 7>15\t\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8>12\t 9>13\t10>14\t11>15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8>10\t 9>11\t12>14\t13>15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8>9\t10>11\t12>13\t14>15\t\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def bitonic_network(n):\n",
    "    \"\"\"Check the computation of a bitonic network\"\"\"\n",
    "    layers = int(np.log2(n))          \n",
    "    for layer in range(1, layers+1):                \n",
    "        for sub in reversed(range(1, layer+1)):\n",
    "            for i in range(0,n,2**sub):\n",
    "                for j in range(2**(sub-1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix+(2**(sub-1))\n",
    "                    swap = \"<\" if (ix >> layer) & 1 else \">\"\n",
    "                    print(f\"{a:>2}{swap}{b:<d}\", end=\"\\t\")                                    \n",
    "            print()\n",
    "        print(\"-\"*n*4)    \n",
    "        \n",
    "# this should match the diagram at the top of the notebook\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕<<┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕<<┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  \n",
      "┕>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕<<<<<┙  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>>>>┙  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<<<<┙  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕<<┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕<<┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  \n",
      "┕>>>>>>>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕<<<<<<<<<<<┙  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>>>>┙  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕<<<<<┙  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<<<<┙  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕<<┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  \n",
      "┕>>>>>>>>>>>>>>>>>>>>>>>┙  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>>>>>>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>>>>>>>>>>┙  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>>>>┙  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>>>>┙  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕>>>>>┙  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕>>┙  \n"
     ]
    }
   ],
   "source": [
    "def pretty_bitonic_network(n):\n",
    "    \"\"\"Pretty print a bitonic network,\n",
    "    to check the logic is correct\"\"\"\n",
    "    layers = int(np.log2(n))  \n",
    "    # header\n",
    "    for i in range(n):\n",
    "        print(f\"{i:<2d} \", end='')\n",
    "    print()\n",
    "    \n",
    "    # layers\n",
    "    for layer in range(1, layers+1):                \n",
    "        for sub in reversed(range(1, layer+1)):\n",
    "            for i in range(0,n,2**sub):\n",
    "                for j in range(2**(sub-1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix+(2**(sub-1))                    \n",
    "                    way = \"<\" if (ix >> layer) & 1 else \">\"\n",
    "                    \n",
    "                    # this could be neater...\n",
    "                    for i in range(n):           \n",
    "                        if i==b:\n",
    "                            print(\"┙\", end='')                                                \n",
    "                        elif i==a:\n",
    "                            print(\"┕\", end='')                                                \n",
    "                        elif not(a < i < b):\n",
    "                            print(\"│\", end='')\n",
    "                        else:\n",
    "                            print(way, end='')\n",
    "                        if a <= i < b:\n",
    "                            print(way*2, end='')\n",
    "                        else:\n",
    "                            print(\" \"*2, end='')\n",
    "                    print()\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "pretty_bitonic_network(16)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(a,b):\n",
    "    \"\"\"The softmaximum of softmax(a,b) = log(e^a + a^b).\"\"\"\n",
    "    return np.log(np.exp(a)+np.exp(b))\n",
    "\n",
    "def softmin(a,b):\n",
    "    \"\"\"\n",
    "    Return the soft-minimum of a and b\n",
    "    The soft-minimum can be derived directly from softmax(a,b).\n",
    "    \"\"\"\n",
    "    return -softmax(-a, -b)\n",
    "\n",
    "def softrank(a, b):\n",
    "    \"\"\"Return a,b in 'soft-sorted' order, with the smaller value first\"\"\"\n",
    "    return softmin(a,b), softmax(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitonic_matrices(n):\n",
    "    \"\"\"Compute a set of bitonic sort matrices to sort a sequence of\n",
    "    length n. n *must* be a power of 2.\n",
    "    \n",
    "    See: https://en.wikipedia.org/wiki/Bitonic_sorter\n",
    "    \n",
    "    Set k=log2(n).\n",
    "    There will be k \"layers\", i=1, 2, ... k\n",
    "    \n",
    "    Each ith layer will have i sub-steps, so there are (k*(k+1)) / 2 sorting steps total.\n",
    "    \n",
    "    For each step, we compute 4 matrices. l and r are binary matrices of size (k/2, k) and\n",
    "    map_l and map_r are matrices of size (k, k/2).\n",
    "    \n",
    "    l and r \"interleave\" the inputs into two k/2 size vectors. map_l and map_r \"uninterleave\" these two k/2 vectors\n",
    "    back into two k sized vectors that can be summed to get the correct output.\n",
    "                    \n",
    "    The result is such that to apply any layer's sorting, we can perform:\n",
    "    \n",
    "    l, r, map_l, map_r = layer[j]\n",
    "    a, b =  l @ y, r @ y                \n",
    "    permuted = map_l @ np.minimum(a, b) + map_r @ np.maximum(a,b)\n",
    "        \n",
    "    Applying this operation for each layer in sequence sorts the input vector.\n",
    "            \n",
    "    \"\"\"\n",
    "    # number of outer layers\n",
    "    layers = int(np.log2(n))\n",
    "    matrices = []\n",
    "    for layer in range(1, layers+1):\n",
    "        # we have 1..layer sub layers\n",
    "        for sub in reversed(range(1, layer+1)):\n",
    "            l, r = np.zeros((n//2, n)), np.zeros((n//2, n))            \n",
    "            map_l, map_r = np.zeros((n, n//2)), np.zeros((n, n//2))                        \n",
    "            out = 0 \n",
    "            for i in range(0,n,2**sub):\n",
    "                for j in range(2**(sub-1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix+(2**(sub-1))                    \n",
    "                    l[out, a] = 1\n",
    "                    r[out, b] = 1\n",
    "                    if (ix >> layer) & 1:                                            \n",
    "                        a, b = b,a                                                            \n",
    "                    map_l[a, out] = 1\n",
    "                    map_r[b, out] = 1                                        \n",
    "                    out += 1\n",
    "            matrices.append((l, r, map_l, map_r))            \n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisort(matrices, x):\n",
    "    \"\"\"\n",
    "    Given a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Sorts exactly.\n",
    "    \"\"\"    \n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = l @ x, r @ x                \n",
    "        x = map_l @ np.minimum(a,b) + map_r @ np.maximum(a,b)        \n",
    "    return x\n",
    "\n",
    "def diff_bisort(matrices, x):    \n",
    "    \"\"\"\n",
    "    Approximate differentiable sort. Takes a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Values will be distorted slightly but will be ordered.\n",
    "    \"\"\"  \n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = softrank(l @ x, r @ x)        \n",
    "        x = map_l @ a + map_r @ b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.  39.  46.  84.  87. 103. 178. 190.]\n",
      "[  0.  36.  47.  66.  93. 107. 150. 171.]\n",
      "[ 16.  38.  45.  63.  89. 126. 137. 170.]\n",
      "[  4.  58.  76. 107. 112. 123. 160. 166.]\n",
      "[  5.  14.  25.  42.  72. 103. 149. 192.]\n",
      "[  1.   3.  58.  59. 114. 139. 147. 161.]\n",
      "[ 13.  31.  41.  52.  56.  92. 102. 130.]\n",
      "[ 23.  40.  41.  54. 112. 119. 195. 197.]\n",
      "[ 13.  76.  78.  99. 124. 136. 172. 172.]\n",
      "[ 13.  15.  38.  84. 103. 106. 112. 127.]\n"
     ]
    }
   ],
   "source": [
    "# Test sorting\n",
    "matrices = bitonic_matrices(8)        \n",
    "for i in range(10):\n",
    "    test = np.random.randint(0,200,8)\n",
    "    print(bisort(matrices, test))\n",
    "    assert(np.allclose(bisort(matrices, test), np.sort(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sorting for 2 elements\n",
      "Testing sorting for 4 elements\n",
      "Testing sorting for 8 elements\n",
      "Testing sorting for 16 elements\n",
      "Testing sorting for 32 elements\n",
      "Testing sorting for 64 elements\n",
      "Testing sorting for 128 elements\n",
      "Testing sorting for 256 elements\n",
      "Testing sorting for 512 elements\n",
      "Testing sorting for 1024 elements\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    k = 2 ** i\n",
    "    matrices = bitonic_matrices(k)  \n",
    "    print(f\"Testing sorting for {k} elements\")\n",
    "    for j in range(100):\n",
    "        test = np.random.randint(0, 200, k)\n",
    "        assert(np.allclose(bisort(matrices, test), np.sort(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-106.  -63.  -33.   41.   73.   91.  107.  139.]\n",
      "[-114.  -77.   10.   53.   70.   76.   92.  143.]\n",
      "[-132. -112.  -94.   31.   73.   84.  119.  154.]\n",
      "[-172. -132.  -94.  -72.  -29.   44.  104.  130.]\n",
      "[-134.  -38.    9.   83.  107.  117.  156.  175.]\n",
      "[-195.   -152.   -120.    -52.    -19.     -2.     60.95   64.05]\n",
      "[-193.    -15.     -2.     22.31   23.69   63.     78.     86.  ]\n",
      "[-180. -159. -131.  -78.   36.   56.  137.  179.]\n",
      "[-138.  -88.  -53.  -42.  -35.   52.  146.  179.]\n",
      "[-105.  -29.  -23.  -16.   82.   96.  109.  151.]\n"
     ]
    }
   ],
   "source": [
    "# Differentiable sorting \n",
    "np.set_printoptions(precision=2)\n",
    "matrices = bitonic_matrices(8) \n",
    "for i in range(10):\n",
    "    print(diff_bisort(matrices, np.random.randint(-200,200,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(16)\n",
    "torch_matrices = [[torch.from_numpy(matrix).float().to(device) for matrix in matrix_set] for matrix_set in matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override softmax to use torch tensors\n",
    "def softmax(a,b):\n",
    "    \"\"\"The softmaximum of softmax(a,b) = log(e^a + e^b).\"\"\"\n",
    "    return torch.log(torch.exp(a)+torch.exp(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2104e-01, 3.8757e-05, 2.2428e-02, 1.9859e-01, 1.5259e-03, 3.4902e-04,\n",
      "         1.6809e-03, 2.3111e-03, 2.8201e-04, 2.8588e-01, 3.6264e-02, 6.3002e-04,\n",
      "         3.1380e-05, 1.2835e-01, 3.6958e-04, 2.3259e-04],\n",
      "        [2.7163e-01, 3.5779e-05, 1.9345e-02, 1.7872e-01, 1.4394e-03, 3.2754e-04,\n",
      "         1.6050e-03, 2.2146e-03, 3.1813e-04, 3.4291e-01, 4.1697e-02, 7.1266e-04,\n",
      "         3.4758e-05, 1.3836e-01, 4.0324e-04, 2.5539e-04],\n",
      "        [1.6584e-01, 9.0402e-05, 3.8121e-02, 2.4091e-01, 4.1187e-03, 8.7890e-04,\n",
      "         4.6219e-03, 6.5168e-03, 5.5896e-04, 1.5907e-01, 8.6112e-02, 1.2246e-03,\n",
      "         7.0083e-05, 2.9058e-01, 7.7589e-04, 5.2269e-04],\n",
      "        [1.6807e-01, 8.9644e-05, 3.7758e-02, 2.4639e-01, 4.0916e-03, 8.7654e-04,\n",
      "         4.5988e-03, 6.4769e-03, 5.6263e-04, 1.5814e-01, 8.6429e-02, 1.2373e-03,\n",
      "         7.0285e-05, 2.8390e-01, 7.7929e-04, 5.2365e-04],\n",
      "        [3.0768e-02, 7.6955e-04, 2.3025e-01, 5.5505e-02, 4.5084e-02, 9.6885e-03,\n",
      "         4.8241e-02, 6.5950e-02, 6.0669e-03, 2.3139e-02, 3.8342e-01, 1.3128e-02,\n",
      "         7.0187e-04, 7.2251e-02, 9.3946e-03, 5.6440e-03],\n",
      "        [2.5280e-02, 1.0128e-03, 3.4844e-01, 4.5925e-02, 5.8208e-02, 1.2645e-02,\n",
      "         6.0780e-02, 8.2020e-02, 5.8780e-03, 2.0980e-02, 2.4628e-01, 1.3120e-02,\n",
      "         6.3920e-04, 6.3112e-02, 1.0047e-02, 5.6309e-03],\n",
      "        [8.8218e-03, 2.7544e-03, 1.3173e-01, 1.7041e-02, 1.6787e-01, 4.3121e-02,\n",
      "         1.7786e-01, 2.3201e-01, 2.5901e-02, 4.3473e-03, 5.3620e-02, 5.0540e-02,\n",
      "         3.4632e-03, 1.0063e-02, 4.4683e-02, 2.6169e-02],\n",
      "        [6.5752e-03, 3.4763e-03, 1.1592e-01, 1.2351e-02, 1.7986e-01, 5.5117e-02,\n",
      "         1.7693e-01, 2.0535e-01, 3.0972e-02, 3.6181e-03, 3.4785e-02, 7.0847e-02,\n",
      "         3.2840e-03, 8.6668e-03, 6.0898e-02, 3.1349e-02],\n",
      "        [4.4468e-04, 1.8556e-02, 1.3437e-02, 1.0638e-03, 1.1484e-01, 8.5145e-02,\n",
      "         1.1728e-01, 9.0631e-02, 1.0854e-01, 5.2778e-04, 8.1301e-03, 1.5959e-01,\n",
      "         2.4187e-02, 1.2079e-03, 1.4855e-01, 1.0787e-01],\n",
      "        [4.2886e-04, 1.8531e-02, 1.3211e-02, 1.0286e-03, 1.1278e-01, 8.5966e-02,\n",
      "         1.1419e-01, 8.8855e-02, 1.0827e-01, 5.5019e-04, 8.3420e-03, 1.6368e-01,\n",
      "         2.3771e-02, 1.2410e-03, 1.5144e-01, 1.0772e-01],\n",
      "        [4.4975e-04, 3.2587e-02, 1.1986e-02, 1.0320e-03, 9.9224e-02, 1.2844e-01,\n",
      "         1.0140e-01, 7.6604e-02, 1.2414e-01, 2.6468e-04, 4.7638e-03, 1.3308e-01,\n",
      "         2.7920e-02, 7.3264e-04, 1.3447e-01, 1.2291e-01],\n",
      "        [3.5393e-04, 3.8014e-02, 1.0052e-02, 8.1099e-04, 8.9463e-02, 1.5649e-01,\n",
      "         8.6528e-02, 6.6307e-02, 1.2302e-01, 2.8995e-04, 4.7415e-03, 1.3778e-01,\n",
      "         2.6279e-02, 7.4526e-04, 1.3722e-01, 1.2190e-01],\n",
      "        [1.0210e-04, 1.0975e-01, 2.4921e-03, 2.1578e-04, 4.2735e-02, 1.4622e-01,\n",
      "         3.6604e-02, 2.6183e-02, 1.7191e-01, 1.0373e-04, 1.9949e-03, 9.2572e-02,\n",
      "         1.1442e-01, 2.8620e-04, 9.8982e-02, 1.5543e-01],\n",
      "        [1.0064e-04, 1.0687e-01, 2.5168e-03, 2.1167e-04, 4.3447e-02, 1.5363e-01,\n",
      "         3.6762e-02, 2.6376e-02, 1.6824e-01, 1.0303e-04, 1.9521e-03, 9.2875e-02,\n",
      "         1.1408e-01, 2.8668e-04, 9.9483e-02, 1.5306e-01],\n",
      "        [5.4582e-05, 3.9861e-01, 1.2835e-03, 1.2035e-04, 1.9455e-02, 6.5044e-02,\n",
      "         1.7274e-02, 1.2332e-02, 6.1649e-02, 3.9356e-05, 7.1983e-04, 3.2667e-02,\n",
      "         2.7022e-01, 1.0505e-04, 4.6285e-02, 7.4148e-02],\n",
      "        [4.1603e-05, 2.6882e-01, 1.0346e-03, 8.9929e-05, 1.5852e-02, 5.6063e-02,\n",
      "         1.3647e-02, 9.8639e-03, 6.3696e-02, 4.2492e-05, 7.4348e-04, 3.6315e-02,\n",
      "         3.9084e-01, 1.1988e-04, 5.6219e-02, 8.6618e-02]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input = np.random.normal(0, 5, 16)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device), requires_grad=True)\n",
    "result = diff_bisort(torch_matrices, var_test_input)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the \n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 16 x 16 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
