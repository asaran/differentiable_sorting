{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations executed in parallel. A sorting network implements  a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$ (sorting networks for non-power-of-2 sizes are possible but not trickier).\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network for $n=2^k$ elements has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a $k$ element vector into sorted order.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification!) as the continuous approximation to the `max(a,b)` function:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b) \\approx \\max(a,b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin(a,b)` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b}) \\approx \\min(a,b).$$\n",
    "\n",
    "These functions obviously aren't equal to max and min, but are relatively close, and differentiable. Note that we now have a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{high} = \\text{softmax}(a,b), \\text{low} = \\text{softmin}(a,b), \\text{where } \\text{low}\\leq \\text{high}$$\n",
    "\n",
    "## Differentiable sorting\n",
    "\n",
    "For each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides which can be done simultaneously. We can any write function that selects the relevant elements of the vector as a multiply with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ and $R \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{k}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ and $R' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i[\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i})] + R'_i[\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})]$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is clearly differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network. \n",
    "\n",
    "This process is excessively computation heavy, but simple -- we could simplify this into two matrix multiplies, at the cost of a vector split and join in the middle. \n",
    "\n",
    "## Example\n",
    "\n",
    "To sort four elements, we have a network like:\n",
    "\n",
    "    0  1  2  3  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕<<┙  \n",
    "    ┕>>>>>┙  │  \n",
    "    │  │  │  │  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕>>┙  \n",
    "    \n",
    "This is equivalent to: \n",
    "\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[3], x[2] = cswap(x[2], x[3])\n",
    "    x[0], x[2] = cswap(x[0], x[2])\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[2], x[3] = cswap(x[2], x[3])\n",
    "    \n",
    "where `cswap(a,b) = (min(a,b), max(a,b))`\n",
    "\n",
    "Replacing the indexing with matrix multiplies and `cswap` with a `softcswap = (softmin(a,b), softmax(a,b))` we then have the differentiable form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitonic_tests import bitonic_network, pretty_bitonic_network\n",
    "\n",
    "# this should match the diagram at the top of the notebook\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_bitonic_network(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sorting\n",
    "import numpy as np\n",
    "from differentiable_sorting import bitonic_matrices, diff_bisort, diff_rank, softmax, softmin, softcswap, bisort\n",
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "for i in range(10):\n",
    "    # these should all be in sorted order\n",
    "    test = np.random.randint(0, 200, 8)\n",
    "    print(bisort(matrices, test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    k = 2**i\n",
    "    matrices = bitonic_matrices(k)\n",
    "    print(f\"Testing sorting for {k} elements\")\n",
    "    for j in range(100):\n",
    "        test = np.random.randint(0, 200, k)\n",
    "        assert (np.allclose(bisort(matrices, test), np.sort(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable sorting \n",
    "np.set_printoptions(precision=2)\n",
    "matrices = bitonic_matrices(8) \n",
    "def neat_vec(n):\n",
    "    return \"\\t\".join([f\"{x:.2f}\" for x in n])\n",
    "\n",
    "for i in range(10):\n",
    "    test = np.random.randint(-200,200,8)\n",
    "    print(\"Differentiable\", neat_vec(diff_bisort(matrices, test)))\n",
    "    print(\"Exact sorting \", neat_vec(bisort(matrices, test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relaxed sorting\n",
    "We can define a slighly modified function which interpolates between `softmax(a,b)` and `mean(a,b)`. The result is a sorting function that can be relaxed from sorting to averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting import diff_bisort_smooth\n",
    "# Differentiable smoothed sorting \n",
    "test = np.random.randint(-200,200,8)\n",
    "print(f\"Mean {np.mean(test)}\")\n",
    "print()\n",
    "print(\"Exact sorting       \", neat_vec(bisort(matrices, test)))\n",
    "for smooth in np.linspace(0, 1, 8):    \n",
    "    print(f\"Diff. smooth[{smooth:.2f}]  \", neat_vec(diff_bisort_smooth(matrices, test, smooth)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woven form\n",
    "We can \"weave\" the four matrices into two matrices for fewer multiplies at the cost of having to split and join the matrices at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting import diff_bisort_weave, bisort_woven_matrices\n",
    "\n",
    "woven_matrices = bisort_woven_matrices(8)\n",
    "\n",
    "print(\"Exact sorting       \", neat_vec(bisort(matrices, test)))\n",
    "print(f\"Diff. (std.)       \", neat_vec(diff_bisort(matrices, test)))\n",
    "print(f\"Diff. (woven)      \", neat_vec(diff_bisort_weave(woven_matrices, test)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable ranking\n",
    "We can use a differentiable similarity measure between the input and output of the vector, e.g. an RBF kernel. We can use this to generate a normalised similarity matrix and apply this to a vector `[1, 2, 3, ..., n]`. This gives a differentiable ranking function.\n",
    "\n",
    "As `sigma` gets larger, the result converges to giving all values the mean rank; as it goes to zero the result converges to the true rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting import order_matrix, diff_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(8)\n",
    "test = np.random.randint(0, 200, 8)\n",
    "sortd = diff_bisort(matrices, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smoothed ranks\")\n",
    "for sigma in [0.1, 1, 10, 100, 1000]:\n",
    "    similarity = order_matrix(test, sortd, sigma=sigma)    \n",
    "    ranks = np.arange(len(test))    \n",
    "    print(f\"sigma={sigma:7.1f}  |\", neat_vec(similarity @ ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_rank(matrices, x, sigma=0.1):\n",
    "    \"\"\"Return the smoothed, differentiable ranking of each element of x. Sigma\n",
    "    specifies the smoothing of the ranking. \"\"\"\n",
    "    sortd = diff_bisort(matrices, x)\n",
    "    return order_matrix(x, sortd, sigma=sigma) @ np.arange(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [5.0, -1.0, 9.5, 13.2, 16.2, 20.5, 42.0, 18.0]\n",
    "np.set_printoptions(suppress=True)\n",
    "print(neat_vec((diff_rank(matrices, x, sigma=0.1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differentiable_sorting_torch import diff_bisort, bitonic_matrices, diff_rank\n",
    "matrices = bitonic_matrices(16)\n",
    "torch_matrices = [[torch.from_numpy(matrix).float().to(device) for matrix in matrix_set] for matrix_set in matrices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.random.normal(0, 5, 16)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device),\n",
    "                          requires_grad=True)\n",
    "\n",
    "result = diff_bisort(torch_matrices, var_test_input)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the\n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(\n",
    "        torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 16 x 16 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = diff_rank(torch_matrices, var_test_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
