{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations executed in parallel. A sorting network implements  a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$ (sorting networks for non-power-of-2 sizes are possible but not trickier).\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network for $2^k$ elements has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a $k$ element vector into sorted order.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification!) as the continuous approximation to the `max(a,b)` function:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b) \\approx \\max(a,b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin(a,b)` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b}) \\approx \\min(a,b).$$\n",
    "\n",
    "These functions obviously aren't equal to max and min, but are relatively close, and differentiable. Note that we now have a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{high} = \\text{softmax}(a,b), \\text{low} = \\text{softmin}(a,b), \\text{where } \\text{low}\\leq \\text{high}$$\n",
    "\n",
    "## Differentiable sorting\n",
    "\n",
    "For each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides which can be done simultaneously. We can any write function that selects the relevant elements of the vector as a multiply with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{k \\times \\frac{k}{2}}$ and $R \\in \\mathbb{R}^{k \\times \\frac{k}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{k}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{k}{2} \\times k}$ and $R' \\in \\mathbb{R}^{\\frac{k}{2} \\times k}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i[\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i})] + R'_i[\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})]$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is clearly differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network. \n",
    "\n",
    "## Example\n",
    "\n",
    "To sort four elements, we have a network like:\n",
    "\n",
    "    0  1  2  3  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕<<┙  \n",
    "    ┕>>>>>┙  │  \n",
    "    │  │  │  │  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕>>┙  \n",
    "    \n",
    "This is equivalent to: \n",
    "\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[3], x[2] = cswap(x[2], x[3])\n",
    "    x[0], x[2] = cswap(x[0], x[2])\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[2], x[3] = cswap(x[2], x[3])\n",
    "    \n",
    "where `cswap(a,b) = (min(a,b), max(a,b))`\n",
    "\n",
    "Replacing the indexing with matrix multiplies and `cswap` with a `softcswap = (softmin(a,b), softmax(a,b))` we then have the differentiable form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0>1\t 2<3\t 4>5\t 6<7\t 8>9\t10<11\t12>13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>2\t 1>3\t 4<6\t 5<7\t 8>10\t 9>11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4<5\t 6<7\t 8>9\t10>11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8<12\t 9<13\t10<14\t11<15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8<10\t 9<11\t12<14\t13<15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8<9\t10<11\t12<13\t14<15\t\n",
      "----------------------------------------------------------------\n",
      " 0>8\t 1>9\t 2>10\t 3>11\t 4>12\t 5>13\t 6>14\t 7>15\t\n",
      " 0>4\t 1>5\t 2>6\t 3>7\t 8>12\t 9>13\t10>14\t11>15\t\n",
      " 0>2\t 1>3\t 4>6\t 5>7\t 8>10\t 9>11\t12>14\t13>15\t\n",
      " 0>1\t 2>3\t 4>5\t 6>7\t 8>9\t10>11\t12>13\t14>15\t\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bitonic_network(n):\n",
    "    \"\"\"Check the computation of a bitonic network\"\"\"\n",
    "    layers = int(np.log2(n))\n",
    "    for layer in range(1, layers + 1):\n",
    "        for sub in reversed(range(1, layer + 1)):\n",
    "            for i in range(0, n, 2**sub):\n",
    "                for j in range(2**(sub - 1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix + (2**(sub - 1))\n",
    "                    swap = \"<\" if (ix >> layer) & 1 else \">\"\n",
    "                    print(f\"{a:>2}{swap}{b:<d}\", end=\"\\t\")\n",
    "            print()\n",
    "        print(\"-\" * n * 4)\n",
    "\n",
    "\n",
    "# this should match the diagram at the top of the notebook\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕<<┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕<<┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  \n",
      "┕>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕<<<<<┙  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>>>>┙  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<<<<┙  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕<<┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕<<┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  \n",
      "┕>>>>>>>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕<<<<<<<<<<<┙  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>>>>┙  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕<<<<<┙  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<<<<┙  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕<<┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕<<┙  \n",
      "┕>>>>>>>>>>>>>>>>>>>>>>>┙  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>>>>>>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>>>>>>>>>>┙  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>>>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>>>>┙  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>>>>┙  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕>>>>>┙  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  ┕>>┙  │  │  \n",
      "│  │  │  │  │  │  │  │  │  │  │  │  │  │  ┕>>┙  \n"
     ]
    }
   ],
   "source": [
    "def pretty_bitonic_network(n):\n",
    "    \"\"\"Pretty print a bitonic network,\n",
    "    to check the logic is correct\"\"\"\n",
    "    layers = int(np.log2(n))\n",
    "    # header\n",
    "    for i in range(n):\n",
    "        print(f\"{i:<2d} \", end='')\n",
    "    print()\n",
    "\n",
    "    # layers\n",
    "    for layer in range(1, layers + 1):\n",
    "        for sub in reversed(range(1, layer + 1)):\n",
    "            for i in range(0, n, 2**sub):\n",
    "                for j in range(2**(sub - 1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix + (2**(sub - 1))\n",
    "                    way = \"<\" if (ix >> layer) & 1 else \">\"\n",
    "\n",
    "                    # this could be neater...\n",
    "                    for i in range(n):\n",
    "                        if i == b:\n",
    "                            print(\"┙\", end='')\n",
    "                        elif i == a:\n",
    "                            print(\"┕\", end='')\n",
    "                        elif not (a < i < b):\n",
    "                            print(\"│\", end='')\n",
    "                        else:\n",
    "                            print(way, end='')\n",
    "                        if a <= i < b:\n",
    "                            print(way * 2, end='')\n",
    "                        else:\n",
    "                            print(\" \" * 2, end='')\n",
    "                    print()\n",
    "\n",
    "\n",
    "pretty_bitonic_network(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(a, b):\n",
    "    \"\"\"The softmaximum of softmax(a,b) = log(e^a + a^b).\"\"\"\n",
    "    return np.log(np.exp(a) + np.exp(b))\n",
    "\n",
    "\n",
    "def softmin(a, b):\n",
    "    \"\"\"\n",
    "    Return the soft-minimum of a and b\n",
    "    The soft-minimum can be derived directly from softmax(a,b).\n",
    "    \"\"\"\n",
    "    return -softmax(-a, -b)\n",
    "\n",
    "\n",
    "def softrank(a, b):\n",
    "    \"\"\"Return a,b in 'soft-sorted' order, with the smaller value first\"\"\"\n",
    "    return softmin(a, b), softmax(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitonic_matrices(n):\n",
    "    \"\"\"Compute a set of bitonic sort matrices to sort a sequence of\n",
    "    length n. n *must* be a power of 2.\n",
    "    \n",
    "    See: https://en.wikipedia.org/wiki/Bitonic_sorter\n",
    "    \n",
    "    Set k=log2(n).\n",
    "    There will be k \"layers\", i=1, 2, ... k\n",
    "    \n",
    "    Each ith layer will have i sub-steps, so there are (k*(k+1)) / 2 sorting steps total.\n",
    "    \n",
    "    For each step, we compute 4 matrices. l and r are binary matrices of size (k/2, k) and\n",
    "    inv_l and inv_r are matrices of size (k, k/2).\n",
    "    \n",
    "    l and r \"interleave\" the inputs into two k/2 size vectors. inv_l and inv_r \"uninterleave\" these two k/2 vectors\n",
    "    back into two k sized vectors that can be summed to get the correct output.\n",
    "                    \n",
    "    The result is such that to apply any layer's sorting, we can perform:\n",
    "    \n",
    "    l, r, inv_l, inv_r = layer[j]\n",
    "    a, b =  l @ y, r @ y                \n",
    "    permuted = inv_l @ np.minimum(a, b) + inv_r @ np.maximum(a,b)\n",
    "        \n",
    "    Applying this operation for each layer in sequence sorts the input vector.\n",
    "            \n",
    "    \"\"\"\n",
    "    # number of outer layers\n",
    "    layers = int(np.log2(n))\n",
    "    matrices = []\n",
    "    for layer in range(1, layers + 1):\n",
    "        # we have 1..layer sub layers\n",
    "        for sub in reversed(range(1, layer + 1)):\n",
    "            l, r = np.zeros((n // 2, n)), np.zeros((n // 2, n))\n",
    "            inv_l, inv_r = np.zeros((n, n // 2)), np.zeros((n, n // 2))\n",
    "            out = 0\n",
    "            for i in range(0, n, 2**sub):\n",
    "                for j in range(2**(sub - 1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix + (2**(sub - 1))\n",
    "                    l[out, a] = 1\n",
    "                    r[out, b] = 1\n",
    "                    if (ix >> layer) & 1:\n",
    "                        a, b = b, a\n",
    "                    inv_l[a, out] = 1\n",
    "                    inv_r[b, out] = 1\n",
    "                    out += 1\n",
    "            matrices.append((l, r, inv_l, inv_r))\n",
    "    return matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisort(matrices, x):\n",
    "    \"\"\"\n",
    "    Given a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Sorts exactly.\n",
    "    \"\"\"\n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = l @ x, r @ x\n",
    "        x = map_l @ np.minimum(a, b) + map_r @ np.maximum(a, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def diff_bisort(matrices, x):\n",
    "    \"\"\"\n",
    "    Approximate differentiable sort. Takes a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Values will be distorted slightly but will be ordered.\n",
    "    \"\"\"\n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = softrank(l @ x, r @ x)\n",
    "        x = map_l @ a + map_r @ b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.  35.  39. 111. 122. 128. 164. 186.]\n",
      "[  3.  27.  38.  93. 124. 148. 168. 195.]\n",
      "[ 32.  36.  38.  45.  72.  72.  87. 180.]\n",
      "[  3.   5.  18.  58.  66. 106. 107. 127.]\n",
      "[ 11.  20.  55.  59. 129. 162. 178. 198.]\n",
      "[ 15.  24.  52.  90. 147. 152. 162. 179.]\n",
      "[  9.  18.  66.  81. 116. 168. 186. 192.]\n",
      "[  6.  32.  42.  49.  59. 107. 107. 145.]\n",
      "[ 23.  34.  60.  66. 108. 128. 135. 152.]\n",
      "[ 26.  34.  64.  73.  99. 107. 141. 197.]\n"
     ]
    }
   ],
   "source": [
    "# Test sorting\n",
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "for i in range(10):\n",
    "    # these should all be in sorted order\n",
    "    test = np.random.randint(0, 200, 8)\n",
    "    print(bisort(matrices, test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sorting for 2 elements\n",
      "Testing sorting for 4 elements\n",
      "Testing sorting for 8 elements\n",
      "Testing sorting for 16 elements\n",
      "Testing sorting for 32 elements\n",
      "Testing sorting for 64 elements\n",
      "Testing sorting for 128 elements\n",
      "Testing sorting for 256 elements\n",
      "Testing sorting for 512 elements\n",
      "Testing sorting for 1024 elements\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    k = 2**i\n",
    "    matrices = bitonic_matrices(k)\n",
    "    print(f\"Testing sorting for {k} elements\")\n",
    "    for j in range(100):\n",
    "        test = np.random.randint(0, 200, k)\n",
    "        assert (np.allclose(bisort(matrices, test), np.sort(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differentiable -190.00\t-160.00\t-50.02\t-45.98\t-37.00\t13.00\t153.00\t161.00\n",
      "Exact sorting  -190.00\t-160.00\t-50.00\t-46.00\t-37.00\t13.00\t153.00\t161.00\n",
      "\n",
      "Differentiable -184.00\t-162.00\t-123.00\t-102.00\t-34.00\t-21.00\t-14.00\t75.00\n",
      "Exact sorting  -184.00\t-162.00\t-123.00\t-102.00\t-34.00\t-21.00\t-14.00\t75.00\n",
      "\n",
      "Differentiable -123.02\t-118.98\t-66.00\t-32.00\t44.00\t63.00\t94.87\t97.13\n",
      "Exact sorting  -123.00\t-119.00\t-66.00\t-32.00\t44.00\t63.00\t95.00\t97.00\n",
      "\n",
      "Differentiable -186.00\t-145.00\t-64.31\t-62.69\t-26.00\t38.00\t136.00\t175.00\n",
      "Exact sorting  -186.00\t-145.00\t-64.00\t-63.00\t-26.00\t38.00\t136.00\t175.00\n",
      "\n",
      "Differentiable -200.00\t-17.00\t38.00\t70.00\t123.00\t130.00\t144.00\t152.00\n",
      "Exact sorting  -200.00\t-17.00\t38.00\t70.00\t123.00\t130.00\t144.00\t152.00\n",
      "\n",
      "Differentiable -142.00\t-133.00\t54.00\t82.00\t99.99\t104.96\t108.05\t120.00\n",
      "Exact sorting  -142.00\t-133.00\t54.00\t82.00\t100.00\t105.00\t108.00\t120.00\n",
      "\n",
      "Differentiable -185.00\t-156.13\t-153.87\t-140.00\t-88.00\t-67.00\t104.00\t134.00\n",
      "Exact sorting  -185.00\t-156.00\t-154.00\t-140.00\t-88.00\t-67.00\t104.00\t134.00\n",
      "\n",
      "Differentiable -119.23\t-116.77\t-86.00\t-61.00\t-49.00\t-41.00\t25.00\t72.00\n",
      "Exact sorting  -119.00\t-117.00\t-86.00\t-61.00\t-49.00\t-41.00\t25.00\t72.00\n",
      "\n",
      "Differentiable -182.00\t-93.69\t-92.31\t3.00\t37.00\t97.00\t135.51\t137.49\n",
      "Exact sorting  -182.00\t-93.00\t-93.00\t3.00\t37.00\t97.00\t136.00\t137.00\n",
      "\n",
      "Differentiable -198.04\t-193.96\t-46.00\t-18.00\t4.00\t30.00\t104.98\t110.02\n",
      "Exact sorting  -198.00\t-194.00\t-46.00\t-18.00\t4.00\t30.00\t105.00\t110.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differentiable sorting \n",
    "np.set_printoptions(precision=2)\n",
    "matrices = bitonic_matrices(8) \n",
    "def neat_vec(n):\n",
    "    return \"\\t\".join([f\"{x:.2f}\" for x in n])\n",
    "\n",
    "for i in range(10):\n",
    "    test = np.random.randint(-200,200,8)\n",
    "    print(\"Differentiable\", neat_vec(diff_bisort(matrices, test)))\n",
    "    print(\"Exact sorting \", neat_vec(bisort(matrices, test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(16)\n",
    "torch_matrices = [[torch.from_numpy(matrix).float().to(device) for matrix in matrix_set] for matrix_set in matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override softmax to use torch tensors\n",
    "def softmax(a, b):\n",
    "    \"\"\"The softmaximum of softmax(a,b) = log(e^a + e^b).\"\"\"\n",
    "    return torch.log(torch.exp(a) + torch.exp(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5800e-08, 1.8749e-08, 3.3806e-05, 4.6596e-07, 1.1410e-04, 5.9777e-03,\n",
      "         1.8987e-04, 8.2044e-02, 2.2655e-04, 4.6479e-06, 9.8265e-04, 9.1464e-07,\n",
      "         4.2966e-05, 9.1038e-01, 2.3056e-07, 4.2127e-09],\n",
      "        [3.3739e-07, 1.3609e-07, 2.1835e-04, 3.2274e-06, 9.4283e-04, 4.3112e-02,\n",
      "         1.6340e-03, 8.6150e-01, 6.8937e-04, 1.8300e-05, 5.1409e-03, 3.4280e-06,\n",
      "         1.4065e-04, 8.6599e-02, 6.8678e-07, 1.2289e-08],\n",
      "        [8.3022e-06, 3.3489e-06, 6.4338e-03, 8.6423e-05, 1.5105e-02, 6.9114e-01,\n",
      "         2.2909e-02, 3.7742e-02, 4.0939e-02, 7.8800e-04, 1.7737e-01, 1.4511e-04,\n",
      "         5.3704e-03, 1.9177e-03, 3.9163e-05, 6.7272e-07],\n",
      "        [1.5567e-05, 6.2741e-06, 1.2063e-02, 1.6179e-04, 2.8746e-02, 2.0295e-01,\n",
      "         4.3566e-02, 1.5883e-02, 8.3621e-02, 1.7831e-03, 5.9980e-01, 3.4132e-04,\n",
      "         1.0057e-02, 9.4583e-04, 6.1377e-05, 1.0538e-06],\n",
      "        [1.4671e-04, 5.4957e-05, 9.7047e-02, 1.6404e-03, 1.6013e-01, 1.7751e-02,\n",
      "         2.1403e-01, 9.3366e-04, 3.2895e-01, 1.0441e-02, 7.9573e-02, 2.2982e-03,\n",
      "         8.6197e-02, 5.0772e-05, 7.4132e-04, 1.1718e-05],\n",
      "        [1.5350e-04, 5.7503e-05, 1.0110e-01, 1.7022e-03, 1.7059e-01, 1.6971e-02,\n",
      "         2.3049e-01, 8.9655e-04, 2.9995e-01, 9.9585e-03, 7.9330e-02, 2.2063e-03,\n",
      "         8.5802e-02, 5.1124e-05, 7.1863e-04, 1.1206e-05],\n",
      "        [3.6946e-04, 1.4677e-04, 2.2945e-01, 3.5687e-03, 2.1524e-01, 9.8600e-03,\n",
      "         1.9351e-01, 4.7142e-04, 9.8997e-02, 2.4908e-02, 2.2712e-02, 5.8388e-03,\n",
      "         1.9334e-01, 2.1844e-05, 1.5414e-03, 2.7565e-05],\n",
      "        [4.0574e-04, 1.6148e-04, 2.2462e-01, 3.7260e-03, 2.0709e-01, 8.9474e-03,\n",
      "         1.8472e-01, 4.4037e-04, 9.2842e-02, 2.4782e-02, 2.3433e-02, 5.5869e-03,\n",
      "         2.2167e-01, 2.1728e-05, 1.5247e-03, 2.5486e-05],\n",
      "        [3.0271e-03, 1.4164e-03, 2.4317e-01, 1.7251e-02, 1.4972e-01, 2.2202e-03,\n",
      "         7.6578e-02, 5.8219e-05, 3.7053e-02, 1.2411e-01, 8.5427e-03, 4.6184e-02,\n",
      "         2.7393e-01, 7.0620e-06, 1.6285e-02, 4.4743e-04],\n",
      "        [6.0422e-03, 2.6847e-03, 6.2056e-02, 5.3334e-02, 4.1877e-02, 8.5923e-04,\n",
      "         2.5633e-02, 2.7102e-05, 1.1438e-02, 4.9277e-01, 2.2797e-03, 1.5448e-01,\n",
      "         9.5547e-02, 2.1241e-06, 4.9560e-02, 1.4062e-03],\n",
      "        [4.8088e-02, 2.2231e-02, 9.1889e-03, 2.3292e-01, 4.6929e-03, 9.0187e-05,\n",
      "         2.9701e-03, 2.8677e-06, 2.3414e-03, 1.4128e-01, 3.8246e-04, 3.1477e-01,\n",
      "         1.3629e-02, 3.0579e-07, 2.0262e-01, 4.7858e-03],\n",
      "        [5.0462e-02, 2.1853e-02, 9.4173e-03, 3.1902e-01, 4.3833e-03, 9.1086e-05,\n",
      "         2.9035e-03, 2.9482e-06, 1.9962e-03, 1.2401e-01, 3.1677e-04, 2.5138e-01,\n",
      "         1.1570e-02, 2.5659e-07, 1.9742e-01, 5.1694e-03],\n",
      "        [1.8134e-01, 1.1168e-01, 2.9138e-03, 1.9231e-01, 7.4505e-04, 1.6938e-05,\n",
      "         4.7941e-04, 5.5441e-07, 5.6526e-04, 2.3534e-02, 7.7307e-05, 1.2889e-01,\n",
      "         1.3957e-03, 3.3288e-08, 3.3418e-01, 2.1870e-02],\n",
      "        [3.8133e-01, 2.1628e-01, 1.6970e-03, 1.2458e-01, 4.8808e-04, 1.1119e-05,\n",
      "         3.0647e-04, 3.5508e-07, 3.2715e-04, 1.8319e-02, 4.9072e-05, 7.5694e-02,\n",
      "         1.0322e-03, 2.6223e-08, 1.4943e-01, 3.0455e-02],\n",
      "        [2.3263e-01, 4.5428e-01, 4.1044e-04, 3.5231e-02, 8.0269e-05, 1.9723e-06,\n",
      "         5.4639e-05, 6.7266e-08, 3.4406e-05, 1.7861e-03, 4.8068e-06, 6.9278e-03,\n",
      "         1.3261e-04, 3.1877e-09, 2.2218e-02, 2.4621e-01],\n",
      "        [9.5980e-02, 1.6915e-01, 1.8104e-04, 1.4462e-02, 4.4816e-05, 1.0014e-06,\n",
      "         2.8389e-05, 3.3970e-08, 2.6205e-05, 1.5109e-03, 4.1799e-06, 5.2434e-03,\n",
      "         1.3511e-04, 2.9475e-09, 2.3655e-02, 6.8958e-01]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input = np.random.normal(0, 5, 16)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device),\n",
    "                          requires_grad=True)\n",
    "result = diff_bisort(torch_matrices, var_test_input)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the\n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(\n",
    "        torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 16 x 16 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
