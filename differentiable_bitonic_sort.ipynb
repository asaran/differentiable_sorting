{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable bitonic sort\n",
    "\n",
    "[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) allow creation of sorting networks with a sequence of fixed conditional swapping operations executed in parallel. A sorting network implements  a map from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where $n=2^k$ (sorting networks for non-power-of-2 sizes are possible but not trickier).\n",
    "\n",
    "<img src=\"BitonicSort1.svg.png\">\n",
    "\n",
    "*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n",
    "\n",
    "The sorting network for $n=2^k$ elements has $\\frac{k(k-1)}{2}$ \"layers\" where parallel compare-and-swap operations are used to rearrange a $k$ element vector into sorted order.\n",
    "\n",
    "### Differentiable compare-and-swap\n",
    "\n",
    "If we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification!) as the continuous approximation to the `max(a,b)` function:\n",
    "\n",
    "$$\\text{softmax}(a,b) = \\log(e^a + e^b) \\approx \\max(a,b).$$\n",
    "\n",
    "We can then fairly obviously write `softmin(a,b)` as:\n",
    "\n",
    "$$\\text{softmin}(a,b) = -\\log(e^{-a} + e^{-b}) \\approx \\min(a,b).$$\n",
    "\n",
    "These functions obviously aren't equal to max and min, but are relatively close, and differentiable. Note that we now have a differentiable compare-and-swap operation:\n",
    "\n",
    "$$\\text{high} = \\text{softmax}(a,b), \\text{low} = \\text{softmin}(a,b), \\text{where } \\text{low}\\leq \\text{high}$$\n",
    "\n",
    "## Differentiable sorting\n",
    "\n",
    "For each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides which can be done simultaneously. We can any write function that selects the relevant elements of the vector as a multiply with a binary matrix.\n",
    "\n",
    "For each layer, we can derive two binary matrices $L \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ and $R \\in \\mathbb{R}^{n \\times \\frac{n}{2}}$ which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two $\\frac{k}{2}$ length vectors. We can also derive two matrices $L' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ and $R' \\in \\mathbb{R}^{\\frac{n}{2} \\times n}$ which put the results of the compare-and-swap operation back into the right positions.\n",
    "\n",
    "Then, each layer $i$ of the sorting process is just:\n",
    "$${\\bf x}_{i+1} = L'_i[\\text{softmin}(L_i{\\bf x_i}, R_i{\\bf x_i})] + R'_i[\\text{softmax}(L_i{\\bf x_i}, R_i{\\bf x_i})]$$\n",
    "$$ = L'_i\\left(-\\log\\left(e^{-L_i{\\bf x}_i} + e^{-R_i{\\bf x}_i}\\right)\\right) +  R'_i\\left(\\log\\left(e^{L_i{\\bf x}_i} + e^{R_i{\\bf x}_i}\\right)\\right)$$\n",
    "which is clearly differentiable (though not very numerically stable -- the usable range of elements $x$ is quite limited in single float precision).\n",
    "\n",
    "All that remains is to compute the matrices $L_i, R_i, L'_i, R'_i$ for each of the layers of the network. \n",
    "\n",
    "## Example\n",
    "\n",
    "To sort four elements, we have a network like:\n",
    "\n",
    "    0  1  2  3  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕<<┙  \n",
    "    ┕>>>>>┙  │  \n",
    "    │  │  │  │  \n",
    "    ┕>>┙  │  │  \n",
    "    │  │  ┕>>┙  \n",
    "    \n",
    "This is equivalent to: \n",
    "\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[3], x[2] = cswap(x[2], x[3])\n",
    "    x[0], x[2] = cswap(x[0], x[2])\n",
    "    x[0], x[1] = cswap(x[0], x[1])\n",
    "    x[2], x[3] = cswap(x[2], x[3])\n",
    "    \n",
    "where `cswap(a,b) = (min(a,b), max(a,b))`\n",
    "\n",
    "Replacing the indexing with matrix multiplies and `cswap` with a `softcswap = (softmin(a,b), softmax(a,b))` we then have the differentiable form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bitonic_network(n):\n",
    "    \"\"\"Check the computation of a bitonic network\"\"\"\n",
    "    layers = int(np.log2(n))\n",
    "    for layer in range(1, layers + 1):\n",
    "        for sub in reversed(range(1, layer + 1)):\n",
    "            for i in range(0, n, 2**sub):\n",
    "                for j in range(2**(sub - 1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix + (2**(sub - 1))\n",
    "                    swap = \"<\" if (ix >> layer) & 1 else \">\"\n",
    "                    print(f\"{a:>2}{swap}{b:<d}\", end=\"\\t\")\n",
    "            print()\n",
    "        print(\"-\" * n * 4)\n",
    "\n",
    "\n",
    "# this should match the diagram at the top of the notebook\n",
    "bitonic_network(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_bitonic_network(n):\n",
    "    \"\"\"Pretty print a bitonic network,\n",
    "    to check the logic is correct\"\"\"\n",
    "    layers = int(np.log2(n))\n",
    "    # header\n",
    "    for i in range(n):\n",
    "        print(f\"{i:<2d} \", end='')\n",
    "    print()\n",
    "\n",
    "    # layers\n",
    "    for layer in range(1, layers + 1):\n",
    "        for sub in reversed(range(1, layer + 1)):\n",
    "            for i in range(0, n, 2**sub):\n",
    "                for j in range(2**(sub - 1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix + (2**(sub - 1))\n",
    "                    way = \"<\" if (ix >> layer) & 1 else \">\"\n",
    "\n",
    "                    # this could be neater...\n",
    "                    for i in range(n):\n",
    "                        if i == b:\n",
    "                            print(\"┙\", end='')\n",
    "                        elif i == a:\n",
    "                            print(\"┕\", end='')\n",
    "                        elif not (a < i < b):\n",
    "                            print(\"│\", end='')\n",
    "                        else:\n",
    "                            print(way, end='')\n",
    "                        if a <= i < b:\n",
    "                            print(way * 2, end='')\n",
    "                        else:\n",
    "                            print(\" \" * 2, end='')\n",
    "                    print()\n",
    "\n",
    "\n",
    "pretty_bitonic_network(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(a, b):\n",
    "    \"\"\"The softmaximum of softmax(a,b) = log(e^a + a^b).\"\"\"\n",
    "    return np.log(np.exp(a) + np.exp(b))\n",
    "\n",
    "\n",
    "def softmin(a, b):\n",
    "    \"\"\"\n",
    "    Return the soft-minimum of a and b\n",
    "    The soft-minimum can be derived directly from softmax(a,b).\n",
    "    \"\"\"\n",
    "    return -softmax(-a, -b)\n",
    "\n",
    "\n",
    "def softrank(a, b):\n",
    "    \"\"\"Return a,b in 'soft-sorted' order, with the smaller value first\"\"\"\n",
    "    return softmin(a, b), softmax(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitonic_matrices(n):\n",
    "    \"\"\"Compute a set of bitonic sort matrices to sort a sequence of\n",
    "    length n. n *must* be a power of 2.\n",
    "    \n",
    "    See: https://en.wikipedia.org/wiki/Bitonic_sorter\n",
    "    \n",
    "    Set k=log2(n).\n",
    "    There will be k \"layers\", i=1, 2, ... k\n",
    "    \n",
    "    Each ith layer will have i sub-steps, so there are (k*(k+1)) / 2 sorting steps total.\n",
    "    \n",
    "    For each step, we compute 4 matrices. l and r are binary matrices of size (k/2, k) and\n",
    "    inv_l and inv_r are matrices of size (k, k/2).\n",
    "    \n",
    "    l and r \"interleave\" the inputs into two k/2 size vectors. inv_l and inv_r \"uninterleave\" these two k/2 vectors\n",
    "    back into two k sized vectors that can be summed to get the correct output.\n",
    "                    \n",
    "    The result is such that to apply any layer's sorting, we can perform:\n",
    "    \n",
    "    l, r, inv_l, inv_r = layer[j]\n",
    "    a, b =  l @ y, r @ y                \n",
    "    permuted = inv_l @ np.minimum(a, b) + inv_r @ np.maximum(a,b)\n",
    "        \n",
    "    Applying this operation for each layer in sequence sorts the input vector.\n",
    "            \n",
    "    \"\"\"\n",
    "    # number of outer layers\n",
    "    layers = int(np.log2(n))\n",
    "    matrices = []\n",
    "    for layer in range(1, layers + 1):\n",
    "        # we have 1..layer sub layers\n",
    "        for sub in reversed(range(1, layer + 1)):\n",
    "            l, r = np.zeros((n // 2, n)), np.zeros((n // 2, n))\n",
    "            inv_l, inv_r = np.zeros((n, n // 2)), np.zeros((n, n // 2))\n",
    "            out = 0\n",
    "            for i in range(0, n, 2**sub):\n",
    "                for j in range(2**(sub - 1)):\n",
    "                    ix = i + j\n",
    "                    a, b = ix, ix + (2**(sub - 1))\n",
    "                    l[out, a] = 1\n",
    "                    r[out, b] = 1\n",
    "                    if (ix >> layer) & 1:\n",
    "                        a, b = b, a\n",
    "                    inv_l[a, out] = 1\n",
    "                    inv_r[b, out] = 1\n",
    "                    out += 1\n",
    "            matrices.append((l, r, inv_l, inv_r))\n",
    "    return matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisort(matrices, x):\n",
    "    \"\"\"\n",
    "    Given a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Sorts exactly.\n",
    "    \"\"\"\n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = l @ x, r @ x\n",
    "        x = map_l @ np.minimum(a, b) + map_r @ np.maximum(a, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def diff_bisort(matrices, x):\n",
    "    \"\"\"\n",
    "    Approximate differentiable sort. Takes a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Values will be distorted slightly but will be ordered.\n",
    "    \"\"\"\n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = softrank(l @ x, r @ x)\n",
    "        x = map_l @ a + map_r @ b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sorting\n",
    "matrices = bitonic_matrices(8)\n",
    "\n",
    "for i in range(10):\n",
    "    # these should all be in sorted order\n",
    "    test = np.random.randint(0, 200, 8)\n",
    "    print(bisort(matrices, test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    k = 2**i\n",
    "    matrices = bitonic_matrices(k)\n",
    "    print(f\"Testing sorting for {k} elements\")\n",
    "    for j in range(100):\n",
    "        test = np.random.randint(0, 200, k)\n",
    "        assert (np.allclose(bisort(matrices, test), np.sort(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable sorting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable sorting \n",
    "np.set_printoptions(precision=2)\n",
    "matrices = bitonic_matrices(8) \n",
    "def neat_vec(n):\n",
    "    return \"\\t\".join([f\"{x:.2f}\" for x in n])\n",
    "\n",
    "for i in range(10):\n",
    "    test = np.random.randint(-200,200,8)\n",
    "    print(\"Differentiable\", neat_vec(diff_bisort(matrices, test)))\n",
    "    print(\"Exact sorting \", neat_vec(bisort(matrices, test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relaxed sorting\n",
    "We can define a slighly modified function which interpolates between `softmax(a,b)` and `mean(a,b)`. The result is a sorting function that can be relaxed from sorting to averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_smooth(a, b, smooth=0):\n",
    "    \"\"\"The smoothed softmaximum of softmax(a,b) = log(e^a + a^b).\n",
    "    With smooth=0.0, is softmax; with smooth=1.0, averages a and b\"\"\"\n",
    "    t = smooth / 2.0\n",
    "    return np.log(np.exp((1-t) * a + b * t) + np.exp((1-t)*b + t *a) ) - np.log(1+smooth)\n",
    "\n",
    "def softrank_smooth(a, b, smooth=0):\n",
    "    \"\"\"The smoothed compare and swap of a and b\n",
    "    With smooth=0, if softrank; with smooth=1.0, geometrically averages a and b\"\"\"\n",
    "    return -softmax_smooth(-a, -b, smooth), softmax_smooth(a, b, smooth)\n",
    "\n",
    "def diff_bisort_smooth(matrices, x, smooth=0):\n",
    "    \"\"\"\n",
    "    Approximate differentiable sort. Takes a set of bitonic sort matrices generated by bitonic_matrices(n), sort \n",
    "    a sequence x of length n. Values will be distorted slightly but will be ordered.\n",
    "    \"\"\"\n",
    "    for l, r, map_l, map_r in matrices:\n",
    "        a, b = softrank_smooth (l @ x, r @ x, smooth)\n",
    "        x = map_l @ a + map_r @ b\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable smoothed sorting \n",
    "test = np.random.randint(-200,200,8)\n",
    "print(f\"Mean {np.mean(test)}\")\n",
    "print()\n",
    "print(\"Exact sorting       \", neat_vec(bisort(matrices, test)))\n",
    "for smooth in np.linspace(0, 1, 8):    \n",
    "    print(f\"Diff. smooth[{smooth:.2f}]  \", neat_vec(diff_bisort_smooth(matrices, test, smooth)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth ranking\n",
    "We can use a differentiable similarity measure between the input and output of the vector, e.g. an RBF kernel. This gives a similarity matrix which we can then apply (traditional) softmax to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_matrix(original, sortd, sigma=0.1):\n",
    "    diff = np.subtract.outer(original, sortd)**2\n",
    "    rbf = np.exp(-(diff**2) / (2*sigma**2))\n",
    "    return (rbf.T / np.sum(rbf, axis=1)).T\n",
    "    return rbf\n",
    "\n",
    "# this is the traditional softmax e^x_i / (sum e^x_i) applied to rows\n",
    "def softmax_matrix(matrix):\n",
    "    e = np.exp(matrix)\n",
    "    return (e.T / np.sum(e, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(4)\n",
    "test = [18, 1, 9, 2]\n",
    "sortd = diff_bisort(matrices, test)\n",
    "similarity = order_matrix(test, sortd, sigma=1)\n",
    "print(similarity)\n",
    "plt.imshow(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax_matrix(similarity))\n",
    "print(np.sum(softmax_matrix(similarity), axis=1))\n",
    "plt.imshow(softmax_matrix(similarity), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch example\n",
    "We can verify that this is both parallelisable on the GPU and fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = bitonic_matrices(16)\n",
    "torch_matrices = [[torch.from_numpy(matrix).float().to(device) for matrix in matrix_set] for matrix_set in matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override softmax to use torch tensors\n",
    "def softmax(a, b):\n",
    "    \"\"\"The softmaximum of softmax(a,b) = log(e^a + e^b).\"\"\"\n",
    "    return torch.log(torch.exp(a) + torch.exp(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.random.normal(0, 5, 16)\n",
    "var_test_input = Variable(torch.from_numpy(test_input).float().to(device),\n",
    "                          requires_grad=True)\n",
    "result = diff_bisort(torch_matrices, var_test_input)\n",
    "\n",
    "# compute the Jacobian of the sorting function, to show we can differentiate through the\n",
    "# sorting function\n",
    "jac = []\n",
    "for i in range(len(result)):\n",
    "    jac.append(\n",
    "        torch.autograd.grad(result[i], var_test_input, retain_graph=True)[0])\n",
    "\n",
    "# 16 x 16 jacobian of the sorting matrix\n",
    "print(torch.stack(jac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
